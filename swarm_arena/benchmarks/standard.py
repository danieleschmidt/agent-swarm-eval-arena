"""Standard benchmarking suite for agent evaluation."""

import time\nimport numpy as np\nfrom typing import List, Dict, Any, Type, Optional\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom ..core.arena import Arena, SimulationResults\nfrom ..core.config import SwarmConfig\nfrom ..core.agent import BaseAgent, Agent, CooperativeAgent, CompetitiveAgent\nfrom ..core.environment import ForagingEnvironment, PursuitEvasionEnvironment\nfrom ..exceptions import SimulationError\nfrom ..utils.logging import get_logger\nfrom ..utils.seeding import set_global_seed\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass BenchmarkResult:\n    \"\"\"Result from a single benchmark run.\"\"\"\n    \n    agent_name: str\n    environment_name: str\n    seed: int\n    mean_reward: float\n    std_reward: float\n    efficiency: float\n    fairness_index: float\n    survival_rate: float\n    cooperation_score: float\n    total_steps: int\n    execution_time: float\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"agent_name\": self.agent_name,\n            \"environment_name\": self.environment_name,\n            \"seed\": self.seed,\n            \"mean_reward\": self.mean_reward,\n            \"std_reward\": self.std_reward,\n            \"efficiency\": self.efficiency,\n            \"fairness_index\": self.fairness_index,\n            \"survival_rate\": self.survival_rate,\n            \"cooperation_score\": self.cooperation_score,\n            \"total_steps\": self.total_steps,\n            \"execution_time\": self.execution_time\n        }\n\n\nclass StandardBenchmark:\n    \"\"\"Standard benchmarking suite for multi-agent evaluation.\"\"\"\n    \n    def __init__(self, \n                 base_config: Optional[SwarmConfig] = None,\n                 num_episodes: int = 10,\n                 num_seeds: int = 5,\n                 parallel_execution: bool = True,\n                 max_workers: int = 4) -> None:\n        \"\"\"Initialize benchmark suite.\n        \n        Args:\n            base_config: Base configuration for benchmarks\n            num_episodes: Episodes per benchmark run\n            num_seeds: Number of random seeds to test\n            parallel_execution: Whether to run benchmarks in parallel\n            max_workers: Maximum parallel workers\n        \"\"\"\n        self.base_config = base_config or SwarmConfig(\n            num_agents=50,\n            episode_length=500,\n            arena_size=(800, 600)\n        )\n        self.num_episodes = num_episodes\n        self.num_seeds = num_seeds\n        self.parallel_execution = parallel_execution\n        self.max_workers = max_workers\n        \n        # Results storage\n        self.results: List[BenchmarkResult] = []\n        \n        logger.info(f\"Standard benchmark initialized (episodes={num_episodes}, seeds={num_seeds})\")\n    \n    def run_single_benchmark(self, \n                           agent_class: Type[BaseAgent],\n                           environment_class: Type,\n                           seed: int,\n                           config_overrides: Optional[Dict[str, Any]] = None) -> BenchmarkResult:\n        \"\"\"Run a single benchmark configuration.\n        \n        Args:\n            agent_class: Agent class to benchmark\n            environment_class: Environment class to use\n            seed: Random seed\n            config_overrides: Configuration overrides\n            \n        Returns:\n            Benchmark result\n        \"\"\"\n        try:\n            # Set global seed\n            set_global_seed(seed)\n            \n            # Create configuration\n            config = self.base_config.copy(seed=seed)\n            if config_overrides:\n                for key, value in config_overrides.items():\n                    if hasattr(config, key):\n                        setattr(config, key, value)\n            \n            # Create environment and arena\n            environment = environment_class(config)\n            arena = Arena(config, environment)\n            \n            # Add agents\n            arena.add_agents(agent_class, count=config.num_agents)\n            \n            # Run benchmark\n            start_time = time.time()\n            results = arena.run(episodes=self.num_episodes, verbose=False)\n            execution_time = time.time() - start_time\n            \n            # Calculate metrics\n            metrics = self._calculate_metrics(results, arena)\n            \n            # Create benchmark result\n            benchmark_result = BenchmarkResult(\n                agent_name=agent_class.__name__,\n                environment_name=environment_class.__name__,\n                seed=seed,\n                execution_time=execution_time,\n                **metrics\n            )\n            \n            logger.debug(f\"Completed benchmark: {agent_class.__name__} in {environment_class.__name__} (seed={seed})\")\n            return benchmark_result\n            \n        except Exception as e:\n            logger.error(f\"Benchmark failed: {agent_class.__name__} in {environment_class.__name__} (seed={seed}): {str(e)}\")\n            # Return failed result\n            return BenchmarkResult(\n                agent_name=agent_class.__name__,\n                environment_name=environment_class.__name__,\n                seed=seed,\n                mean_reward=0.0,\n                std_reward=0.0,\n                efficiency=0.0,\n                fairness_index=0.0,\n                survival_rate=0.0,\n                cooperation_score=0.0,\n                total_steps=0,\n                execution_time=0.0\n            )\n    \n    def run_all(self, \n               agent_classes: List[Type[BaseAgent]],\n               environments: Optional[List[str]] = None,\n               metrics: Optional[List[str]] = None) -> List[BenchmarkResult]:\n        \"\"\"Run comprehensive benchmark suite.\n        \n        Args:\n            agent_classes: List of agent classes to benchmark\n            environments: List of environment names\n            metrics: List of metrics to compute\n            \n        Returns:\n            List of benchmark results\n        \"\"\"\n        try:\n            if environments is None:\n                environments = [\"foraging_standard\", \"foraging_sparse\", \"pursuit_evasion\"]\n            \n            if metrics is None:\n                metrics = [\"reward\", \"efficiency\", \"fairness\", \"survival\", \"cooperation\"]\n            \n            # Define environment configurations\n            env_configs = {\n                \"foraging_standard\": (ForagingEnvironment, {}),\n                \"foraging_sparse\": (ForagingEnvironment, {\"resource_spawn_rate\": 0.02}),\n                \"foraging_dense\": (ForagingEnvironment, {\"resource_spawn_rate\": 0.2}),\n                \"pursuit_evasion\": (PursuitEvasionEnvironment, {\"environment_config\": {\"predator_ratio\": 0.1}})\n            }\n            \n            # Generate all benchmark configurations\n            benchmark_configs = []\n            for agent_class in agent_classes:\n                for env_name in environments:\n                    if env_name not in env_configs:\n                        logger.warning(f\"Unknown environment: {env_name}\")\n                        continue\n                    \n                    env_class, config_overrides = env_configs[env_name]\n                    \n                    for seed in range(self.num_seeds):\n                        benchmark_configs.append((\n                            agent_class,\n                            env_class,\n                            seed,\n                            config_overrides\n                        ))\n            \n            logger.info(f\"Running {len(benchmark_configs)} benchmark configurations\")\n            \n            # Execute benchmarks\n            if self.parallel_execution and len(benchmark_configs) > 1:\n                results = self._run_parallel_benchmarks(benchmark_configs)\n            else:\n                results = self._run_sequential_benchmarks(benchmark_configs)\n            \n            self.results.extend(results)\n            \n            logger.info(f\"Completed {len(results)} benchmarks\")\n            return results\n            \n        except Exception as e:\n            raise SimulationError(f\"Benchmark suite execution failed: {str(e)}\")\n    \n    def _run_parallel_benchmarks(self, configs: List[tuple]) -> List[BenchmarkResult]:\n        \"\"\"Run benchmarks in parallel.\"\"\"\n        results = []\n        \n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Submit all benchmark jobs\n            future_to_config = {\n                executor.submit(\n                    self.run_single_benchmark,\n                    agent_class, env_class, seed, config_overrides\n                ): (agent_class.__name__, env_class.__name__, seed)\n                for agent_class, env_class, seed, config_overrides in configs\n            }\n            \n            # Collect results as they complete\n            completed = 0\n            for future in as_completed(future_to_config):\n                try:\n                    result = future.result()\n                    results.append(result)\n                    completed += 1\n                    \n                    if completed % 10 == 0:\n                        logger.info(f\"Completed {completed}/{len(configs)} benchmarks\")\n                        \n                except Exception as e:\n                    agent_name, env_name, seed = future_to_config[future]\n                    logger.error(f\"Parallel benchmark failed: {agent_name} in {env_name} (seed={seed}): {str(e)}\")\n        \n        return results\n    \n    def _run_sequential_benchmarks(self, configs: List[tuple]) -> List[BenchmarkResult]:\n        \"\"\"Run benchmarks sequentially.\"\"\"\n        results = []\n        \n        for i, (agent_class, env_class, seed, config_overrides) in enumerate(configs):\n            result = self.run_single_benchmark(agent_class, env_class, seed, config_overrides)\n            results.append(result)\n            \n            if (i + 1) % 10 == 0:\n                logger.info(f\"Completed {i + 1}/{len(configs)} benchmarks\")\n        \n        return results\n    \n    def _calculate_metrics(self, results: SimulationResults, arena: Arena) -> Dict[str, float]:\n        \"\"\"Calculate benchmark metrics from simulation results.\"\"\"\n        try:\n            # Basic reward metrics\n            all_rewards = []\n            for agent_rewards in results.episode_rewards.values():\n                all_rewards.extend(agent_rewards)\n            \n            mean_reward = np.mean(all_rewards) if all_rewards else 0.0\n            std_reward = np.std(all_rewards) if all_rewards else 0.0\n            \n            # Efficiency metric (resources collected per agent)\n            total_resources = sum(\n                stats[\"resources_collected\"] \n                for stats in results.agent_stats.values()\n            )\n            efficiency = total_resources / len(arena.agents) if arena.agents else 0.0\n            \n            # Fairness index (from results or calculate Gini coefficient)\n            fairness_index = results.fairness_index if results.fairness_index is not None else 0.0\n            \n            # Survival rate\n            alive_agents = sum(\n                1 for stats in results.agent_stats.values() \n                if stats.get(\"alive\", True)\n            )\n            survival_rate = alive_agents / len(arena.agents) if arena.agents else 0.0\n            \n            # Cooperation score (based on agent clustering/coordination)\n            cooperation_score = self._calculate_cooperation_score(results, arena)\n            \n            return {\n                \"mean_reward\": float(mean_reward),\n                \"std_reward\": float(std_reward),\n                \"efficiency\": float(efficiency),\n                \"fairness_index\": float(fairness_index),\n                \"survival_rate\": float(survival_rate),\n                \"cooperation_score\": float(cooperation_score),\n                \"total_steps\": results.total_steps\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error calculating metrics: {str(e)}\")\n            return {\n                \"mean_reward\": 0.0,\n                \"std_reward\": 0.0,\n                \"efficiency\": 0.0,\n                \"fairness_index\": 0.0,\n                \"survival_rate\": 0.0,\n                \"cooperation_score\": 0.0,\n                \"total_steps\": 0\n            }\n    \n    def _calculate_cooperation_score(self, results: SimulationResults, arena: Arena) -> float:\n        \"\"\"Calculate cooperation score based on agent behavior.\"\"\"\n        try:\n            # Simple cooperation metric based on reward variance\n            # Lower variance indicates more cooperative behavior\n            agent_totals = [sum(rewards) for rewards in results.episode_rewards.values()]\n            \n            if len(agent_totals) < 2:\n                return 0.0\n            \n            mean_total = np.mean(agent_totals)\n            if mean_total == 0:\n                return 0.0\n            \n            # Normalized inverse of coefficient of variation\n            cv = np.std(agent_totals) / mean_total\n            cooperation_score = max(0.0, 1.0 - cv)\n            \n            return float(cooperation_score)\n            \n        except Exception as e:\n            logger.error(f\"Error calculating cooperation score: {str(e)}\")\n            return 0.0\n    \n    def get_summary_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics across all benchmark results.\"\"\"\n        if not self.results:\n            return {\"status\": \"no_results\", \"message\": \"No benchmark results available\"}\n        \n        try:\n            # Group results by agent and environment\n            grouped = {}\n            for result in self.results:\n                key = (result.agent_name, result.environment_name)\n                if key not in grouped:\n                    grouped[key] = []\n                grouped[key].append(result)\n            \n            # Calculate statistics for each group\n            summary = {}\n            for (agent_name, env_name), group_results in grouped.items():\n                key = f\"{agent_name}_{env_name}\"\n                \n                # Extract metrics\n                rewards = [r.mean_reward for r in group_results]\n                efficiencies = [r.efficiency for r in group_results]\n                fairness_scores = [r.fairness_index for r in group_results]\n                \n                summary[key] = {\n                    \"agent\": agent_name,\n                    \"environment\": env_name,\n                    \"num_runs\": len(group_results),\n                    \"mean_reward\": {\n                        \"mean\": np.mean(rewards),\n                        \"std\": np.std(rewards),\n                        \"min\": np.min(rewards),\n                        \"max\": np.max(rewards)\n                    },\n                    \"efficiency\": {\n                        \"mean\": np.mean(efficiencies),\n                        \"std\": np.std(efficiencies)\n                    },\n                    \"fairness\": {\n                        \"mean\": np.mean(fairness_scores),\n                        \"std\": np.std(fairness_scores)\n                    },\n                    \"avg_execution_time\": np.mean([r.execution_time for r in group_results])\n                }\n            \n            return summary\n            \n        except Exception as e:\n            logger.error(f\"Error generating summary statistics: {str(e)}\")\n            return {\"status\": \"error\", \"message\": str(e)}\n    \n    def to_latex_table(self, \n                      results: Optional[List[BenchmarkResult]] = None,\n                      caption: str = \"Benchmark Results\",\n                      label: str = \"tab:benchmark_results\") -> str:\n        \"\"\"Generate LaTeX table from benchmark results.\"\"\"\n        if results is None:\n            results = self.results\n        \n        if not results:\n            return \"% No benchmark results available\"\n        \n        try:\n            # Group and average results\n            grouped = {}\n            for result in results:\n                key = (result.agent_name, result.environment_name)\n                if key not in grouped:\n                    grouped[key] = []\n                grouped[key].append(result)\n            \n            # Generate LaTeX table\n            latex_lines = [\n                \"\\\\begin{table}[htbp]\",\n                \"\\\\centering\",\n                \"\\\\begin{tabular}{|l|l|c|c|c|c|}\",\n                \"\\\\hline\",\n                \"Agent & Environment & Mean Reward & Efficiency & Fairness & Survival \\\\\\\\\",\n                \"\\\\hline\"\n            ]\n            \n            for (agent_name, env_name), group_results in sorted(grouped.items()):\n                mean_reward = np.mean([r.mean_reward for r in group_results])\n                efficiency = np.mean([r.efficiency for r in group_results])\n                fairness = np.mean([r.fairness_index for r in group_results])\n                survival = np.mean([r.survival_rate for r in group_results])\n                \n                line = f\"{agent_name} & {env_name} & {mean_reward:.3f} & {efficiency:.3f} & {fairness:.3f} & {survival:.3f} \\\\\\\\\"\n                latex_lines.append(line)\n            \n            latex_lines.extend([\n                \"\\\\hline\",\n                \"\\\\end{tabular}\",\n                f\"\\\\caption{{{caption}}}\",\n                f\"\\\\label{{{label}}}\",\n                \"\\\\end{table}\"\n            ])\n            \n            return \"\\n\".join(latex_lines)\n            \n        except Exception as e:\n            logger.error(f\"Error generating LaTeX table: {str(e)}\")\n            return f\"% Error generating table: {str(e)}\"\n    \n    def export_results(self, filename: str, format: str = \"json\") -> None:\n        \"\"\"Export benchmark results to file.\"\"\"\n        try:\n            import json\n            \n            if format.lower() == \"json\":\n                data = [result.to_dict() for result in self.results]\n                with open(filename, 'w') as f:\n                    json.dump(data, f, indent=2)\n            \n            elif format.lower() == \"csv\":\n                import csv\n                \n                if not self.results:\n                    return\n                \n                fieldnames = list(self.results[0].to_dict().keys())\n                \n                with open(filename, 'w', newline='') as f:\n                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n                    writer.writeheader()\n                    for result in self.results:\n                        writer.writerow(result.to_dict())\n            \n            else:\n                raise ValueError(f\"Unsupported export format: {format}\")\n            \n            logger.info(f\"Exported {len(self.results)} benchmark results to {filename}\")\n            \n        except Exception as e:\n            logger.error(f\"Error exporting results: {str(e)}\")\n            raise