"""Scaling benchmarks for performance analysis."""

import time\nimport psutil\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom dataclasses import dataclass\nimport multiprocessing as mp\n\nfrom ..core.arena import Arena\nfrom ..core.config import SwarmConfig\nfrom ..core.agent import Agent\nfrom ..core.environment import ForagingEnvironment\nfrom ..exceptions import SimulationError\nfrom ..utils.logging import get_logger\nfrom ..utils.seeding import set_global_seed\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass ScalingResult:\n    \"\"\"Result from scaling benchmark.\"\"\"\n    \n    test_type: str  # \"weak_scaling\" or \"strong_scaling\"\n    scale_factor: int\n    num_agents: int\n    arena_size: Tuple[float, float]\n    num_workers: Optional[int] = None\n    \n    # Performance metrics\n    total_time: float = 0.0\n    avg_step_time: float = 0.0\n    throughput: float = 0.0  # steps/second\n    memory_usage: float = 0.0  # MB\n    cpu_usage: float = 0.0  # %\n    \n    # Efficiency metrics\n    parallel_efficiency: float = 0.0\n    scaling_efficiency: float = 0.0\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"test_type\": self.test_type,\n            \"scale_factor\": self.scale_factor,\n            \"num_agents\": self.num_agents,\n            \"arena_size\": self.arena_size,\n            \"num_workers\": self.num_workers,\n            \"total_time\": self.total_time,\n            \"avg_step_time\": self.avg_step_time,\n            \"throughput\": self.throughput,\n            \"memory_usage\": self.memory_usage,\n            \"cpu_usage\": self.cpu_usage,\n            \"parallel_efficiency\": self.parallel_efficiency,\n            \"scaling_efficiency\": self.scaling_efficiency\n        }\n\n\nclass ScalingBenchmark:\n    \"\"\"Benchmark suite for analyzing scaling properties.\"\"\"\n    \n    def __init__(self, \n                 base_config: Optional[SwarmConfig] = None,\n                 episode_length: int = 200,\n                 warmup_steps: int = 50) -> None:\n        \"\"\"Initialize scaling benchmark.\n        \n        Args:\n            base_config: Base configuration for scaling tests\n            episode_length: Length of episodes for timing\n            warmup_steps: Number of warmup steps before measurement\n        \"\"\"\n        self.base_config = base_config or SwarmConfig(\n            num_agents=100,\n            episode_length=episode_length,\n            arena_size=(1000, 1000),\n            resource_spawn_rate=0.05\n        )\n        self.episode_length = episode_length\n        self.warmup_steps = warmup_steps\n        \n        self.results: List[ScalingResult] = []\n        \n        logger.info(f\"Scaling benchmark initialized (episode_length={episode_length})\")\n    \n    def weak_scaling_test(self, \n                         base_agents: int = 100,\n                         base_arena: float = 1000,\n                         scale_factors: List[int] = None,\n                         metric: str = \"throughput\") -> List[ScalingResult]:\n        \"\"\"Test weak scaling (increase problem size proportionally).\n        \n        In weak scaling, both the number of agents and arena size increase\n        proportionally. Ideal scaling maintains constant execution time.\n        \n        Args:\n            base_agents: Base number of agents\n            base_arena: Base arena dimension\n            scale_factors: List of scaling factors to test\n            metric: Primary metric to optimize for\n            \n        Returns:\n            List of scaling results\n        \"\"\"\n        if scale_factors is None:\n            scale_factors = [1, 2, 4, 8, 16]\n        \n        logger.info(f\"Starting weak scaling test with factors {scale_factors}\")\n        \n        results = []\n        baseline_time = None\n        \n        for scale_factor in scale_factors:\n            try:\n                # Calculate scaled parameters\n                num_agents = base_agents * scale_factor\n                arena_size = (base_arena * np.sqrt(scale_factor), \n                             base_arena * np.sqrt(scale_factor))\n                \n                logger.info(f\"Testing scale factor {scale_factor}: {num_agents} agents, \"\n                           f\"arena {arena_size[0]:.0f}x{arena_size[1]:.0f}\")\n                \n                # Run benchmark\n                result = self._run_single_scaling_test(\n                    \"weak_scaling\",\n                    scale_factor,\n                    num_agents,\n                    arena_size\n                )\n                \n                # Calculate efficiency metrics\n                if baseline_time is None:\n                    baseline_time = result.total_time\n                    result.scaling_efficiency = 1.0\n                else:\n                    # Weak scaling efficiency: T_1 / T_n (should be close to 1.0)\n                    result.scaling_efficiency = baseline_time / result.total_time\n                \n                results.append(result)\n                \n                logger.info(f\"Scale factor {scale_factor}: {result.total_time:.2f}s, \"\n                           f\"efficiency: {result.scaling_efficiency:.3f}\")\n                \n            except Exception as e:\n                logger.error(f\"Weak scaling test failed for scale factor {scale_factor}: {str(e)}\")\n                continue\n        \n        self.results.extend(results)\n        return results\n    \n    def strong_scaling_test(self,\n                           num_agents: int = 1000,\n                           num_workers_list: List[int] = None,\n                           metric: str = \"episode_time\") -> List[ScalingResult]:\n        \"\"\"Test strong scaling (fixed problem size, varying compute resources).\n        \n        In strong scaling, the problem size is fixed while compute resources\n        increase. Ideal scaling shows linear speedup.\n        \n        Args:\n            num_agents: Fixed number of agents\n            num_workers_list: List of worker counts to test\n            metric: Primary metric to optimize for\n            \n        Returns:\n            List of scaling results\n        \"\"\"\n        if num_workers_list is None:\n            max_cores = mp.cpu_count()\n            num_workers_list = [1, 2, 4, min(8, max_cores), min(16, max_cores)]\n            num_workers_list = [w for w in num_workers_list if w <= max_cores]\n        \n        logger.info(f\"Starting strong scaling test with workers {num_workers_list}\")\n        \n        results = []\n        baseline_time = None\n        \n        arena_size = self.base_config.arena_size\n        \n        for num_workers in num_workers_list:\n            try:\n                logger.info(f\"Testing {num_workers} workers with {num_agents} agents\")\n                \n                # For single-threaded case, run without distributed setup\n                if num_workers == 1:\n                    result = self._run_single_scaling_test(\n                        \"strong_scaling\",\n                        num_workers,\n                        num_agents,\n                        arena_size\n                    )\n                else:\n                    # TODO: Implement distributed scaling test\n                    # This would use DistributedArena with multiple workers\n                    logger.warning(f\"Distributed testing not implemented, simulating results for {num_workers} workers\")\n                    \n                    # Simulate distributed performance (for demonstration)\n                    result = self._simulate_distributed_result(\n                        \"strong_scaling\",\n                        num_workers,\n                        num_agents,\n                        arena_size\n                    )\n                \n                result.num_workers = num_workers\n                \n                # Calculate parallel efficiency\n                if baseline_time is None:\n                    baseline_time = result.total_time\n                    result.parallel_efficiency = 1.0\n                else:\n                    # Strong scaling efficiency: (T_1 / T_n) / n\n                    speedup = baseline_time / result.total_time\n                    result.parallel_efficiency = speedup / num_workers\n                \n                results.append(result)\n                \n                logger.info(f\"{num_workers} workers: {result.total_time:.2f}s, \"\n                           f\"efficiency: {result.parallel_efficiency:.3f}\")\n                \n            except Exception as e:\n                logger.error(f\"Strong scaling test failed for {num_workers} workers: {str(e)}\")\n                continue\n        \n        self.results.extend(results)\n        return results\n    \n    def _run_single_scaling_test(self,\n                                test_type: str,\n                                scale_factor: int,\n                                num_agents: int,\n                                arena_size: Tuple[float, float]) -> ScalingResult:\n        \"\"\"Run a single scaling test configuration.\"\"\"\n        try:\n            # Create configuration\n            config = self.base_config.copy(\n                num_agents=num_agents,\n                arena_size=arena_size,\n                episode_length=self.episode_length\n            )\n            \n            # Create arena\n            environment = ForagingEnvironment(config)\n            arena = Arena(config, environment)\n            arena.add_agents(Agent, count=num_agents)\n            \n            # Warm up\n            arena.reset()\n            for _ in range(self.warmup_steps):\n                arena.step()\n            \n            # Measure performance\n            start_time = time.time()\n            start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n            cpu_before = psutil.cpu_percent(interval=None)\n            \n            # Run benchmark steps\n            step_times = []\n            for step in range(self.episode_length):\n                step_start = time.time()\n                observations, rewards, done, info = arena.step()\n                step_time = time.time() - step_start\n                step_times.append(step_time)\n                \n                if done:\n                    break\n            \n            # Final measurements\n            total_time = time.time() - start_time\n            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n            cpu_after = psutil.cpu_percent(interval=None)\n            \n            # Calculate metrics\n            avg_step_time = np.mean(step_times)\n            throughput = len(step_times) / total_time  # steps per second\n            memory_usage = end_memory - start_memory\n            cpu_usage = (cpu_after + cpu_before) / 2  # Approximate\n            \n            # Create result\n            result = ScalingResult(\n                test_type=test_type,\n                scale_factor=scale_factor,\n                num_agents=num_agents,\n                arena_size=arena_size,\n                total_time=total_time,\n                avg_step_time=avg_step_time,\n                throughput=throughput,\n                memory_usage=memory_usage,\n                cpu_usage=cpu_usage\n            )\n            \n            return result\n            \n        except Exception as e:\n            raise SimulationError(f\"Scaling test failed: {str(e)}\")\n    \n    def _simulate_distributed_result(self,\n                                   test_type: str,\n                                   num_workers: int,\n                                   num_agents: int,\n                                   arena_size: Tuple[float, float]) -> ScalingResult:\n        \"\"\"Simulate distributed performance results.\n        \n        This is a placeholder for actual distributed benchmarking.\n        In a real implementation, this would use DistributedArena.\n        \"\"\"\n        # Simulate scaling with some overhead\n        base_time = 10.0  # Baseline time for single worker\n        \n        # Simulate sub-linear speedup due to communication overhead\n        efficiency_factor = 1.0 - (num_workers - 1) * 0.05  # 5% overhead per additional worker\n        simulated_time = base_time / (num_workers * efficiency_factor)\n        \n        # Add some realistic variance\n        simulated_time *= np.random.normal(1.0, 0.05)  # 5% variance\n        \n        result = ScalingResult(\n            test_type=test_type,\n            scale_factor=num_workers,\n            num_agents=num_agents,\n            arena_size=arena_size,\n            total_time=simulated_time,\n            avg_step_time=simulated_time / self.episode_length,\n            throughput=self.episode_length / simulated_time,\n            memory_usage=50 * num_workers,  # Simulate memory scaling\n            cpu_usage=80.0  # Simulate high CPU usage\n        )\n        \n        return result\n    \n    def plot_scaling_curves(self,\n                           weak_scaling_results: List[ScalingResult],\n                           strong_scaling_results: List[ScalingResult],\n                           save_path: str = \"scaling_analysis.png\") -> None:\n        \"\"\"Plot scaling efficiency curves.\"\"\"\n        try:\n            import matplotlib.pyplot as plt\n            \n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n            \n            # Weak scaling plot\n            if weak_scaling_results:\n                scale_factors = [r.scale_factor for r in weak_scaling_results]\n                efficiencies = [r.scaling_efficiency for r in weak_scaling_results]\n                \n                ax1.plot(scale_factors, efficiencies, 'bo-', label='Weak Scaling')\n                ax1.axhline(y=1.0, color='r', linestyle='--', label='Ideal')\n                ax1.set_xlabel('Scale Factor')\n                ax1.set_ylabel('Scaling Efficiency')\n                ax1.set_title('Weak Scaling Efficiency')\n                ax1.legend()\n                ax1.grid(True, alpha=0.3)\n            \n            # Strong scaling plot\n            if strong_scaling_results:\n                num_workers = [r.num_workers for r in strong_scaling_results if r.num_workers]\n                efficiencies = [r.parallel_efficiency for r in strong_scaling_results if r.num_workers]\n                \n                if num_workers:\n                    ax2.plot(num_workers, efficiencies, 'ro-', label='Strong Scaling')\n                    ax2.axhline(y=1.0, color='r', linestyle='--', label='Ideal')\n                    ax2.set_xlabel('Number of Workers')\n                    ax2.set_ylabel('Parallel Efficiency')\n                    ax2.set_title('Strong Scaling Efficiency')\n                    ax2.legend()\n                    ax2.grid(True, alpha=0.3)\n            \n            plt.tight_layout()\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            plt.close()\n            \n            logger.info(f\"Scaling curves saved to {save_path}\")\n            \n        except ImportError:\n            logger.warning(\"Matplotlib not available, cannot plot scaling curves\")\n        except Exception as e:\n            logger.error(f\"Error plotting scaling curves: {str(e)}\")\n    \n    def generate_scaling_report(self) -> str:\n        \"\"\"Generate comprehensive scaling analysis report.\"\"\"\n        if not self.results:\n            return \"No scaling results available.\"\n        \n        try:\n            report_lines = []\n            report_lines.append(\"SCALING ANALYSIS REPORT\")\n            report_lines.append(\"=\" * 50)\n            report_lines.append(\"\")\n            \n            # Separate results by test type\n            weak_results = [r for r in self.results if r.test_type == \"weak_scaling\"]\n            strong_results = [r for r in self.results if r.test_type == \"strong_scaling\"]\n            \n            # Weak scaling analysis\n            if weak_results:\n                report_lines.append(\"WEAK SCALING RESULTS:\")\n                report_lines.append(\"-\" * 30)\n                report_lines.append(f\"{'Scale Factor':<12} {'Agents':<8} {'Time (s)':<10} {'Efficiency':<12} {'Throughput':<12}\")\n                report_lines.append(\"-\" * 70)\n                \n                for result in weak_results:\n                    report_lines.append(\n                        f\"{result.scale_factor:<12} {result.num_agents:<8} \"\n                        f\"{result.total_time:<10.2f} {result.scaling_efficiency:<12.3f} \"\n                        f\"{result.throughput:<12.1f}\"\n                    )\n                \n                # Weak scaling summary\n                avg_efficiency = np.mean([r.scaling_efficiency for r in weak_results])\n                report_lines.append(f\"\\nAverage weak scaling efficiency: {avg_efficiency:.3f}\")\n                report_lines.append(\"\")\n            \n            # Strong scaling analysis\n            if strong_results:\n                report_lines.append(\"STRONG SCALING RESULTS:\")\n                report_lines.append(\"-\" * 30)\n                report_lines.append(f\"{'Workers':<8} {'Time (s)':<10} {'Speedup':<10} {'Efficiency':<12} {'Throughput':<12}\")\n                report_lines.append(\"-\" * 60)\n                \n                baseline_time = strong_results[0].total_time if strong_results else 0\n                \n                for result in strong_results:\n                    speedup = baseline_time / result.total_time if result.total_time > 0 else 0\n                    report_lines.append(\n                        f\"{result.num_workers or 1:<8} {result.total_time:<10.2f} \"\n                        f\"{speedup:<10.2f} {result.parallel_efficiency:<12.3f} \"\n                        f\"{result.throughput:<12.1f}\"\n                    )\n                \n                # Strong scaling summary\n                valid_results = [r for r in strong_results if r.parallel_efficiency > 0]\n                if valid_results:\n                    avg_efficiency = np.mean([r.parallel_efficiency for r in valid_results])\n                    report_lines.append(f\"\\nAverage strong scaling efficiency: {avg_efficiency:.3f}\")\n                report_lines.append(\"\")\n            \n            # Overall recommendations\n            report_lines.append(\"RECOMMENDATIONS:\")\n            report_lines.append(\"-\" * 20)\n            \n            if weak_results:\n                best_weak = max(weak_results, key=lambda r: r.scaling_efficiency)\n                report_lines.append(f\"• Best weak scaling at factor {best_weak.scale_factor} \"\n                                  f\"with {best_weak.scaling_efficiency:.3f} efficiency\")\n            \n            if strong_results:\n                best_strong = max(strong_results, key=lambda r: r.parallel_efficiency)\n                report_lines.append(f\"• Best strong scaling with {best_strong.num_workers} workers \"\n                                  f\"at {best_strong.parallel_efficiency:.3f} efficiency\")\n            \n            return \"\\n\".join(report_lines)\n            \n        except Exception as e:\n            logger.error(f\"Error generating scaling report: {str(e)}\")\n            return f\"Error generating report: {str(e)}\"\n    \n    def export_results(self, filename: str, format: str = \"json\") -> None:\n        \"\"\"Export scaling results to file.\"\"\"\n        try:\n            if format.lower() == \"json\":\n                import json\n                data = [result.to_dict() for result in self.results]\n                with open(filename, 'w') as f:\n                    json.dump(data, f, indent=2)\n            \n            elif format.lower() == \"csv\":\n                import csv\n                if not self.results:\n                    return\n                \n                fieldnames = list(self.results[0].to_dict().keys())\n                with open(filename, 'w', newline='') as f:\n                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n                    writer.writeheader()\n                    for result in self.results:\n                        writer.writerow(result.to_dict())\n            \n            else:\n                raise ValueError(f\"Unsupported format: {format}\")\n            \n            logger.info(f\"Exported {len(self.results)} scaling results to {filename}\")\n            \n        except Exception as e:\n            logger.error(f\"Error exporting scaling results: {str(e)}\")\n            raise