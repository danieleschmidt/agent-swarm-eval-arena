#!/usr/bin/env python3\n\"\"\"Comprehensive benchmarking example.\"\"\"\n\nfrom swarm_arena import SwarmConfig\nfrom swarm_arena.core.agent import Agent, CooperativeAgent, CompetitiveAgent, RandomAgent\nfrom swarm_arena.benchmarks import StandardBenchmark, ScalingBenchmark\n\n\ndef main():\n    \"\"\"Run comprehensive benchmarking example.\"\"\"\n    print(\"ğŸ“Š Swarm Arena Benchmarking Example\")\n    print(\"=\" * 50)\n    \n    # 1. Standard Performance Benchmarks\n    print(\"\\nğŸ Running Standard Benchmarks...\")\n    \n    # Configure benchmark\n    benchmark_config = SwarmConfig(\n        num_agents=50,\n        episode_length=300,\n        arena_size=(800, 600),\n        resource_spawn_rate=0.05\n    )\n    \n    standard_benchmark = StandardBenchmark(\n        base_config=benchmark_config,\n        num_episodes=5,\n        num_seeds=3,\n        parallel_execution=True,\n        max_workers=2\n    )\n    \n    # Define agent classes to benchmark\n    agent_classes = [\n        Agent,\n        CooperativeAgent, \n        CompetitiveAgent,\n        RandomAgent\n    ]\n    \n    # Define environments to test\n    environments = [\n        \"foraging_standard\",\n        \"foraging_sparse\",\n        \"pursuit_evasion\"\n    ]\n    \n    # Run benchmarks\n    print(f\"Testing {len(agent_classes)} agent types across {len(environments)} environments...\")\n    \n    try:\n        results = standard_benchmark.run_all(\n            agent_classes=agent_classes,\n            environments=environments,\n            metrics=[\"reward\", \"efficiency\", \"fairness\", \"cooperation\"]\n        )\n        \n        print(f\"\\nâœ… Completed {len(results)} benchmark runs\")\n        \n        # Display summary statistics\n        summary = standard_benchmark.get_summary_statistics()\n        \n        print(f\"\\nğŸ“ˆ Performance Summary:\")\n        print(\"-\" * 40)\n        \n        for key, stats in summary.items():\n            if isinstance(stats, dict) and \"agent\" in stats:\n                agent_name = stats[\"agent\"]\n                env_name = stats[\"environment\"]\n                mean_reward = stats[\"mean_reward\"][\"mean\"]\n                efficiency = stats[\"efficiency\"][\"mean\"]\n                \n                print(f\"{agent_name:<15} | {env_name:<15} | Reward: {mean_reward:.3f} | Efficiency: {efficiency:.3f}\")\n        \n        # Generate LaTeX table\n        print(f\"\\nğŸ“ LaTeX Table:\")\n        latex_table = standard_benchmark.to_latex_table(\n            results,\n            caption=\"Agent Performance Comparison\",\n            label=\"tab:agent_performance\"\n        )\n        print(latex_table)\n        \n        # Export results\n        standard_benchmark.export_results(\"benchmark_results.json\", \"json\")\n        print(f\"\\nğŸ’¾ Results exported to benchmark_results.json\")\n        \n    except Exception as e:\n        print(f\"âŒ Standard benchmark failed: {str(e)}\")\n    \n    # 2. Scaling Performance Analysis\n    print(f\"\\nâš¡ Running Scaling Analysis...\")\n    \n    scaling_config = SwarmConfig(\n        episode_length=200,\n        resource_spawn_rate=0.05\n    )\n    \n    scaling_benchmark = ScalingBenchmark(\n        base_config=scaling_config,\n        episode_length=200,\n        warmup_steps=20\n    )\n    \n    try:\n        # Weak scaling test (problem size increases with resources)\n        print(\"\\nğŸ”„ Weak Scaling Test:\")\n        weak_results = scaling_benchmark.weak_scaling_test(\n            base_agents=100,\n            base_arena=1000,\n            scale_factors=[1, 2, 4, 8],\n            metric=\"throughput\"\n        )\n        \n        print(f\"Weak scaling results:\")\n        for result in weak_results:\n            print(f\"  Scale {result.scale_factor}x: {result.num_agents} agents, \"\n                  f\"{result.total_time:.2f}s, efficiency: {result.scaling_efficiency:.3f}\")\n        \n        # Strong scaling test (fixed problem size, more workers)\n        print(\"\\nğŸ’ª Strong Scaling Test:\")\n        strong_results = scaling_benchmark.strong_scaling_test(\n            num_agents=500,\n            num_workers_list=[1, 2, 4],\n            metric=\"episode_time\"\n        )\n        \n        print(f\"Strong scaling results:\")\n        for result in strong_results:\n            workers = result.num_workers or 1\n            print(f\"  {workers} workers: {result.total_time:.2f}s, \"\n                  f\"efficiency: {result.parallel_efficiency:.3f}\")\n        \n        # Generate scaling analysis report\n        scaling_report = scaling_benchmark.generate_scaling_report()\n        print(f\"\\nğŸ“‹ Scaling Analysis Report:\")\n        print(scaling_report)\n        \n        # Plot scaling curves (if matplotlib available)\n        try:\n            scaling_benchmark.plot_scaling_curves(\n                weak_results, strong_results, \"scaling_curves.png\"\n            )\n            print(f\"\\nğŸ“Š Scaling curves saved to scaling_curves.png\")\n        except Exception as e:\n            print(f\"\\nâš ï¸  Could not generate plots: {str(e)}\")\n        \n        # Export scaling results\n        scaling_benchmark.export_results(\"scaling_results.json\", \"json\")\n        print(f\"\\nğŸ’¾ Scaling results exported to scaling_results.json\")\n        \n    except Exception as e:\n        print(f\"âŒ Scaling benchmark failed: {str(e)}\")\n    \n    # 3. Performance Recommendations\n    print(f\"\\nğŸ¯ Performance Recommendations:\")\n    print(\"-\" * 40)\n    \n    if 'results' in locals() and results:\n        # Find best performing agent\n        best_result = max(results, key=lambda r: r.mean_reward)\n        print(f\"â€¢ Best overall agent: {best_result.agent_name} \"\n              f\"(reward: {best_result.mean_reward:.3f})\")\n        \n        # Find most efficient agent\n        most_efficient = max(results, key=lambda r: r.efficiency)\n        print(f\"â€¢ Most efficient agent: {most_efficient.agent_name} \"\n              f\"(efficiency: {most_efficient.efficiency:.3f})\")\n        \n        # Find fairest agent\n        fairest = max(results, key=lambda r: r.fairness_index)\n        print(f\"â€¢ Fairest agent: {fairest.agent_name} \"\n              f\"(fairness: {fairest.fairness_index:.3f})\")\n    \n    if 'weak_results' in locals() and weak_results:\n        best_scaling = max(weak_results, key=lambda r: r.scaling_efficiency)\n        print(f\"â€¢ Optimal scale factor: {best_scaling.scale_factor}x \"\n              f\"({best_scaling.num_agents} agents)\")\n    \n    print(f\"\\nâ€¢ For large simulations (1000+ agents), use distributed execution\")\n    print(f\"â€¢ CooperativeAgent typically performs well in resource-sharing scenarios\")\n    print(f\"â€¢ CompetitiveAgent excels in resource-scarce environments\")\n    print(f\"â€¢ Monitor memory usage when scaling beyond 5000 agents\")\n    \n    print(f\"\\nâœ… Benchmarking complete!\")\n\n\nif __name__ == \"__main__\":\n    main()