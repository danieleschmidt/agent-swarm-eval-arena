"""Tests for benchmarking components."""\n\nimport pytest\nimport time\nimport numpy as np\nfrom unittest.mock import Mock, patch, MagicMock\nfrom concurrent.futures import Future\n\nfrom swarm_arena import SwarmConfig\nfrom swarm_arena.core.agent import Agent, CooperativeAgent, CompetitiveAgent, RandomAgent\nfrom swarm_arena.core.arena import SimulationResults\nfrom swarm_arena.benchmarks import StandardBenchmark, ScalingBenchmark\nfrom swarm_arena.benchmarks.standard import BenchmarkResult\nfrom swarm_arena.benchmarks.scaling import ScalingResult\nfrom swarm_arena.exceptions import SimulationError\n\n\nclass TestBenchmarkResult:\n    \"\"\"Test BenchmarkResult dataclass.\"\"\"\n    \n    def test_creation(self):\n        \"\"\"Test benchmark result creation.\"\"\"\n        result = BenchmarkResult(\n            agent_name=\"TestAgent\",\n            environment_name=\"TestEnv\",\n            seed=42,\n            mean_reward=1.5,\n            std_reward=0.3,\n            efficiency=0.8,\n            fairness_index=0.7,\n            survival_rate=0.9,\n            cooperation_score=0.6,\n            total_steps=1000,\n            execution_time=5.2\n        )\n        \n        assert result.agent_name == \"TestAgent\"\n        assert result.environment_name == \"TestEnv\"\n        assert result.seed == 42\n        assert result.mean_reward == 1.5\n        assert result.execution_time == 5.2\n    \n    def test_to_dict(self):\n        \"\"\"Test conversion to dictionary.\"\"\"\n        result = BenchmarkResult(\n            agent_name=\"Agent\",\n            environment_name=\"Env\",\n            seed=1,\n            mean_reward=2.0,\n            std_reward=0.5,\n            efficiency=0.9,\n            fairness_index=0.8,\n            survival_rate=1.0,\n            cooperation_score=0.7,\n            total_steps=500,\n            execution_time=3.0\n        )\n        \n        result_dict = result.to_dict()\n        \n        assert isinstance(result_dict, dict)\n        assert result_dict[\"agent_name\"] == \"Agent\"\n        assert result_dict[\"mean_reward\"] == 2.0\n        assert result_dict[\"total_steps\"] == 500\n\n\nclass TestStandardBenchmark:\n    \"\"\"Test StandardBenchmark class.\"\"\"\n    \n    def test_initialization(self):\n        \"\"\"Test benchmark initialization.\"\"\"\n        config = SwarmConfig(num_agents=20, episode_length=100)\n        benchmark = StandardBenchmark(\n            base_config=config,\n            num_episodes=5,\n            num_seeds=3,\n            parallel_execution=False,\n            max_workers=2\n        )\n        \n        assert benchmark.base_config == config\n        assert benchmark.num_episodes == 5\n        assert benchmark.num_seeds == 3\n        assert benchmark.parallel_execution is False\n        assert benchmark.max_workers == 2\n        assert len(benchmark.results) == 0\n    \n    def test_default_initialization(self):\n        \"\"\"Test benchmark with default parameters.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        assert benchmark.base_config.num_agents == 50\n        assert benchmark.num_episodes == 10\n        assert benchmark.num_seeds == 5\n        assert benchmark.parallel_execution is True\n    \n    @patch('swarm_arena.benchmarks.standard.Arena')\n    def test_run_single_benchmark(self, mock_arena_class):\n        \"\"\"Test running a single benchmark.\"\"\"\n        # Mock arena and results\n        mock_arena = Mock()\n        mock_arena_class.return_value = mock_arena\n        \n        mock_results = Mock(spec=SimulationResults)\n        mock_results.episode_rewards = {0: [1.0, 2.0], 1: [1.5, 2.5]}\n        mock_results.agent_stats = {\n            0: {\"resources_collected\": 5, \"alive\": True},\n            1: {\"resources_collected\": 7, \"alive\": True}\n        }\n        mock_results.total_steps = 200\n        mock_results.fairness_index = 0.8\n        \n        mock_arena.run.return_value = mock_results\n        mock_arena.agents = {0: Mock(), 1: Mock()}\n        \n        benchmark = StandardBenchmark(num_episodes=2, num_seeds=1)\n        \n        result = benchmark.run_single_benchmark(\n            Agent, \n            Mock,  # Mock environment class\n            seed=42\n        )\n        \n        assert isinstance(result, BenchmarkResult)\n        assert result.agent_name == \"Agent\"\n        assert result.seed == 42\n        assert result.mean_reward > 0\n        assert result.efficiency > 0\n        assert result.total_steps == 200\n        assert result.execution_time > 0\n    \n    @patch('swarm_arena.benchmarks.standard.Arena')\n    def test_run_single_benchmark_failure(self, mock_arena_class):\n        \"\"\"Test benchmark failure handling.\"\"\"\n        mock_arena_class.side_effect = Exception(\"Test error\")\n        \n        benchmark = StandardBenchmark()\n        \n        result = benchmark.run_single_benchmark(Agent, Mock, seed=42)\n        \n        # Should return a failed result, not raise exception\n        assert result.agent_name == \"Agent\"\n        assert result.mean_reward == 0.0\n        assert result.efficiency == 0.0\n        assert result.execution_time == 0.0\n    \n    def test_calculate_metrics(self):\n        \"\"\"Test metrics calculation.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        # Mock simulation results\n        results = Mock(spec=SimulationResults)\n        results.episode_rewards = {\n            0: [1.0, 2.0, 1.5],\n            1: [0.5, 1.0, 0.8]\n        }\n        results.agent_stats = {\n            0: {\"resources_collected\": 3, \"alive\": True},\n            1: {\"resources_collected\": 2, \"alive\": False}\n        }\n        results.total_steps = 150\n        results.fairness_index = 0.75\n        \n        # Mock arena\n        mock_arena = Mock()\n        mock_arena.agents = {0: Mock(), 1: Mock()}\n        \n        metrics = benchmark._calculate_metrics(results, mock_arena)\n        \n        assert \"mean_reward\" in metrics\n        assert \"std_reward\" in metrics\n        assert \"efficiency\" in metrics\n        assert \"fairness_index\" in metrics\n        assert \"survival_rate\" in metrics\n        assert \"cooperation_score\" in metrics\n        assert \"total_steps\" in metrics\n        \n        assert metrics[\"fairness_index\"] == 0.75\n        assert metrics[\"total_steps\"] == 150\n        assert metrics[\"survival_rate\"] == 0.5  # 1 out of 2 alive\n        assert metrics[\"efficiency\"] == 2.5  # (3+2)/2 resources per agent\n    \n    def test_calculate_cooperation_score(self):\n        \"\"\"Test cooperation score calculation.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        # Mock results with varying rewards (less cooperative)\n        results1 = Mock(spec=SimulationResults)\n        results1.episode_rewards = {\n            0: [10.0],  # High reward\n            1: [1.0],   # Low reward\n            2: [5.0]    # Medium reward\n        }\n        \n        score1 = benchmark._calculate_cooperation_score(results1, Mock())\n        \n        # Mock results with similar rewards (more cooperative)\n        results2 = Mock(spec=SimulationResults)\n        results2.episode_rewards = {\n            0: [5.0],\n            1: [5.1],\n            2: [4.9]\n        }\n        \n        score2 = benchmark._calculate_cooperation_score(results2, Mock())\n        \n        # More similar rewards should have higher cooperation score\n        assert score2 > score1\n        assert 0 <= score1 <= 1\n        assert 0 <= score2 <= 1\n    \n    @patch('swarm_arena.benchmarks.standard.ThreadPoolExecutor')\n    def test_run_parallel_benchmarks(self, mock_executor_class):\n        \"\"\"Test parallel benchmark execution.\"\"\"\n        benchmark = StandardBenchmark(parallel_execution=True, max_workers=2)\n        \n        # Mock executor and futures\n        mock_executor = Mock()\n        mock_executor_class.return_value.__enter__.return_value = mock_executor\n        \n        # Create mock futures\n        future1 = Mock(spec=Future)\n        future2 = Mock(spec=Future)\n        \n        # Mock benchmark results\n        result1 = BenchmarkResult(\n            \"Agent1\", \"Env1\", 1, 1.0, 0.1, 0.8, 0.7, 0.9, 0.6, 100, 1.0\n        )\n        result2 = BenchmarkResult(\n            \"Agent2\", \"Env2\", 2, 2.0, 0.2, 0.9, 0.8, 1.0, 0.7, 200, 2.0\n        )\n        \n        future1.result.return_value = result1\n        future2.result.return_value = result2\n        \n        mock_executor.submit.side_effect = [future1, future2]\n        \n        # Mock as_completed\n        with patch('swarm_arena.benchmarks.standard.as_completed') as mock_as_completed:\n            mock_as_completed.return_value = [future1, future2]\n            \n            configs = [\n                (Agent, Mock, 1, {}),\n                (CooperativeAgent, Mock, 2, {})\n            ]\n            \n            results = benchmark._run_parallel_benchmarks(configs)\n        \n        assert len(results) == 2\n        assert results[0] == result1\n        assert results[1] == result2\n    \n    def test_run_sequential_benchmarks(self):\n        \"\"\"Test sequential benchmark execution.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        # Mock the run_single_benchmark method\n        result1 = BenchmarkResult(\n            \"Agent\", \"Env\", 1, 1.0, 0.1, 0.8, 0.7, 0.9, 0.6, 100, 1.0\n        )\n        result2 = BenchmarkResult(\n            \"Agent\", \"Env\", 2, 2.0, 0.2, 0.9, 0.8, 1.0, 0.7, 200, 2.0\n        )\n        \n        with patch.object(benchmark, 'run_single_benchmark') as mock_run:\n            mock_run.side_effect = [result1, result2]\n            \n            configs = [\n                (Agent, Mock, 1, {}),\n                (Agent, Mock, 2, {})\n            ]\n            \n            results = benchmark._run_sequential_benchmarks(configs)\n        \n        assert len(results) == 2\n        assert results[0] == result1\n        assert results[1] == result2\n        assert mock_run.call_count == 2\n    \n    def test_get_summary_statistics_no_results(self):\n        \"\"\"Test summary statistics with no results.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        summary = benchmark.get_summary_statistics()\n        \n        assert summary[\"status\"] == \"no_results\"\n        assert \"message\" in summary\n    \n    def test_get_summary_statistics_with_results(self):\n        \"\"\"Test summary statistics with results.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        # Add mock results\n        result1 = BenchmarkResult(\n            \"Agent\", \"Env\", 1, 1.0, 0.1, 0.8, 0.7, 0.9, 0.6, 100, 1.0\n        )\n        result2 = BenchmarkResult(\n            \"Agent\", \"Env\", 2, 1.5, 0.15, 0.85, 0.75, 0.95, 0.65, 150, 1.5\n        )\n        \n        benchmark.results = [result1, result2]\n        \n        summary = benchmark.get_summary_statistics()\n        \n        assert \"Agent_Env\" in summary\n        stats = summary[\"Agent_Env\"]\n        \n        assert stats[\"agent\"] == \"Agent\"\n        assert stats[\"environment\"] == \"Env\"\n        assert stats[\"num_runs\"] == 2\n        assert \"mean_reward\" in stats\n        assert \"efficiency\" in stats\n        assert \"fairness\" in stats\n        assert \"avg_execution_time\" in stats\n    \n    def test_to_latex_table_no_results(self):\n        \"\"\"Test LaTeX table generation with no results.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        latex = benchmark.to_latex_table()\n        \n        assert \"No benchmark results available\" in latex\n    \n    def test_to_latex_table_with_results(self):\n        \"\"\"Test LaTeX table generation with results.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        results = [\n            BenchmarkResult(\n                \"Agent1\", \"Env1\", 1, 1.0, 0.1, 0.8, 0.7, 0.9, 0.6, 100, 1.0\n            ),\n            BenchmarkResult(\n                \"Agent2\", \"Env1\", 2, 2.0, 0.2, 0.9, 0.8, 1.0, 0.7, 200, 2.0\n            )\n        ]\n        \n        latex = benchmark.to_latex_table(\n            results, \n            caption=\"Test Results\", \n            label=\"tab:test\"\n        )\n        \n        assert \"\\\\begin{table}\" in latex\n        assert \"\\\\end{table}\" in latex\n        assert \"Agent1\" in latex\n        assert \"Agent2\" in latex\n        assert \"Test Results\" in latex\n        assert \"tab:test\" in latex\n    \n    @patch('builtins.open', create=True)\n    def test_export_results_json(self, mock_open):\n        \"\"\"Test JSON export of results.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        result = BenchmarkResult(\n            \"Agent\", \"Env\", 1, 1.0, 0.1, 0.8, 0.7, 0.9, 0.6, 100, 1.0\n        )\n        benchmark.results = [result]\n        \n        mock_file = Mock()\n        mock_open.return_value.__enter__.return_value = mock_file\n        \n        benchmark.export_results(\"test.json\", \"json\")\n        \n        mock_open.assert_called_once_with(\"test.json\", 'w')\n        mock_file.write.assert_called()  # JSON should be written\n    \n    @patch('builtins.open', create=True)\n    @patch('csv.DictWriter')\n    def test_export_results_csv(self, mock_dict_writer, mock_open):\n        \"\"\"Test CSV export of results.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        result = BenchmarkResult(\n            \"Agent\", \"Env\", 1, 1.0, 0.1, 0.8, 0.7, 0.9, 0.6, 100, 1.0\n        )\n        benchmark.results = [result]\n        \n        mock_writer = Mock()\n        mock_dict_writer.return_value = mock_writer\n        \n        benchmark.export_results(\"test.csv\", \"csv\")\n        \n        mock_open.assert_called_once_with(\"test.csv\", 'w', newline='')\n        mock_writer.writeheader.assert_called_once()\n        mock_writer.writerow.assert_called_once()\n    \n    def test_export_results_invalid_format(self):\n        \"\"\"Test export with invalid format.\"\"\"\n        benchmark = StandardBenchmark()\n        \n        with pytest.raises(ValueError):\n            benchmark.export_results(\"test.txt\", \"invalid\")\n\n\nclass TestScalingResult:\n    \"\"\"Test ScalingResult dataclass.\"\"\"\n    \n    def test_creation(self):\n        \"\"\"Test scaling result creation.\"\"\"\n        result = ScalingResult(\n            test_type=\"weak_scaling\",\n            scale_factor=4,\n            num_agents=400,\n            arena_size=(2000, 2000),\n            num_workers=4,\n            total_time=10.5,\n            avg_step_time=0.05,\n            throughput=20.0,\n            memory_usage=1024.0,\n            cpu_usage=75.0,\n            parallel_efficiency=0.8,\n            scaling_efficiency=0.9\n        )\n        \n        assert result.test_type == \"weak_scaling\"\n        assert result.scale_factor == 4\n        assert result.num_agents == 400\n        assert result.arena_size == (2000, 2000)\n        assert result.num_workers == 4\n        assert result.parallel_efficiency == 0.8\n    \n    def test_to_dict(self):\n        \"\"\"Test conversion to dictionary.\"\"\"\n        result = ScalingResult(\n            test_type=\"strong_scaling\",\n            scale_factor=2,\n            num_agents=100,\n            arena_size=(1000, 1000)\n        )\n        \n        result_dict = result.to_dict()\n        \n        assert isinstance(result_dict, dict)\n        assert result_dict[\"test_type\"] == \"strong_scaling\"\n        assert result_dict[\"scale_factor\"] == 2\n        assert result_dict[\"num_agents\"] == 100\n        assert result_dict[\"arena_size\"] == (1000, 1000)\n\n\nclass TestScalingBenchmark:\n    \"\"\"Test ScalingBenchmark class.\"\"\"\n    \n    def test_initialization(self):\n        \"\"\"Test scaling benchmark initialization.\"\"\"\n        config = SwarmConfig(num_agents=50, episode_length=100)\n        benchmark = ScalingBenchmark(\n            base_config=config,\n            episode_length=150,\n            warmup_steps=20\n        )\n        \n        assert benchmark.base_config.num_agents == 50\n        assert benchmark.episode_length == 150\n        assert benchmark.warmup_steps == 20\n        assert len(benchmark.results) == 0\n    \n    def test_default_initialization(self):\n        \"\"\"Test scaling benchmark with defaults.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        assert benchmark.base_config.num_agents == 100\n        assert benchmark.episode_length == 200\n        assert benchmark.warmup_steps == 50\n    \n    @patch('swarm_arena.benchmarks.scaling.Arena')\n    @patch('swarm_arena.benchmarks.scaling.psutil')\n    def test_run_single_scaling_test(self, mock_psutil, mock_arena_class):\n        \"\"\"Test running a single scaling test.\"\"\"\n        # Mock psutil\n        mock_process = Mock()\n        mock_process.memory_info.return_value.rss = 1024 * 1024 * 100  # 100 MB\n        mock_psutil.Process.return_value = mock_process\n        mock_psutil.cpu_percent.return_value = 50.0\n        \n        # Mock arena\n        mock_arena = Mock()\n        mock_arena_class.return_value = mock_arena\n        mock_arena.step.return_value = ({}, {}, False, {})\n        \n        benchmark = ScalingBenchmark(episode_length=10, warmup_steps=2)\n        \n        result = benchmark._run_single_scaling_test(\n            \"weak_scaling\",\n            scale_factor=2,\n            num_agents=200,\n            arena_size=(1500, 1500)\n        )\n        \n        assert isinstance(result, ScalingResult)\n        assert result.test_type == \"weak_scaling\"\n        assert result.scale_factor == 2\n        assert result.num_agents == 200\n        assert result.arena_size == (1500, 1500)\n        assert result.total_time > 0\n        assert result.avg_step_time > 0\n        assert result.throughput > 0\n    \n    def test_simulate_distributed_result(self):\n        \"\"\"Test distributed result simulation.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        result = benchmark._simulate_distributed_result(\n            \"strong_scaling\",\n            num_workers=4,\n            num_agents=1000,\n            arena_size=(2000, 2000)\n        )\n        \n        assert isinstance(result, ScalingResult)\n        assert result.test_type == \"strong_scaling\"\n        assert result.scale_factor == 4\n        assert result.num_agents == 1000\n        assert result.total_time > 0\n        assert result.memory_usage > 0\n    \n    def test_weak_scaling_test(self):\n        \"\"\"Test weak scaling test execution.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        # Mock the single test method\n        with patch.object(benchmark, '_run_single_scaling_test') as mock_run:\n            mock_results = [\n                ScalingResult(\n                    \"weak_scaling\", i, 100*i, (1000*i, 1000*i), \n                    total_time=10.0 + i, scaling_efficiency=1.0 - i*0.1\n                )\n                for i in [1, 2, 4]\n            ]\n            mock_run.side_effect = mock_results\n            \n            results = benchmark.weak_scaling_test(\n                base_agents=100,\n                base_arena=1000,\n                scale_factors=[1, 2, 4]\n            )\n        \n        assert len(results) == 3\n        assert all(r.test_type == \"weak_scaling\" for r in results)\n        assert results[0].scaling_efficiency == 1.0  # Baseline\n        assert mock_run.call_count == 3\n    \n    def test_strong_scaling_test(self):\n        \"\"\"Test strong scaling test execution.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        # Mock single test for 1 worker\n        single_result = ScalingResult(\n            \"strong_scaling\", 1, 1000, (2000, 2000),\n            total_time=20.0, parallel_efficiency=1.0\n        )\n        \n        # Mock distributed results for multiple workers\n        distributed_results = [\n            ScalingResult(\n                \"strong_scaling\", i, 1000, (2000, 2000),\n                total_time=20.0/i*0.8, parallel_efficiency=0.8/i  # Simulate efficiency loss\n            )\n            for i in [2, 4]\n        ]\n        \n        with patch.object(benchmark, '_run_single_scaling_test') as mock_single, \\\n             patch.object(benchmark, '_simulate_distributed_result') as mock_dist:\n            \n            mock_single.return_value = single_result\n            mock_dist.side_effect = distributed_results\n            \n            results = benchmark.strong_scaling_test(\n                num_agents=1000,\n                num_workers_list=[1, 2, 4]\n            )\n        \n        assert len(results) == 3\n        assert all(r.test_type == \"strong_scaling\" for r in results)\n        assert results[0].parallel_efficiency == 1.0  # Baseline\n    \n    @patch('matplotlib.pyplot')\n    def test_plot_scaling_curves(self, mock_plt):\n        \"\"\"Test scaling curve plotting.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        weak_results = [\n            ScalingResult(\"weak_scaling\", 1, 100, (1000, 1000), scaling_efficiency=1.0),\n            ScalingResult(\"weak_scaling\", 2, 200, (1400, 1400), scaling_efficiency=0.9)\n        ]\n        \n        strong_results = [\n            ScalingResult(\"strong_scaling\", 1, 1000, (2000, 2000), num_workers=1, parallel_efficiency=1.0),\n            ScalingResult(\"strong_scaling\", 2, 1000, (2000, 2000), num_workers=2, parallel_efficiency=0.8)\n        ]\n        \n        # Should not raise exception\n        benchmark.plot_scaling_curves(weak_results, strong_results, \"test.png\")\n        \n        # Verify matplotlib was called\n        mock_plt.subplots.assert_called_once()\n        mock_plt.tight_layout.assert_called_once()\n        mock_plt.savefig.assert_called_once_with(\"test.png\", dpi=300, bbox_inches='tight')\n    \n    def test_generate_scaling_report_no_results(self):\n        \"\"\"Test scaling report with no results.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        report = benchmark.generate_scaling_report()\n        \n        assert \"No scaling results available\" in report\n    \n    def test_generate_scaling_report_with_results(self):\n        \"\"\"Test scaling report generation with results.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        # Add mock results\n        benchmark.results = [\n            ScalingResult(\n                \"weak_scaling\", 1, 100, (1000, 1000),\n                total_time=10.0, scaling_efficiency=1.0, throughput=10.0\n            ),\n            ScalingResult(\n                \"weak_scaling\", 2, 200, (1400, 1400),\n                total_time=11.0, scaling_efficiency=0.9, throughput=18.0\n            ),\n            ScalingResult(\n                \"strong_scaling\", 1, 1000, (2000, 2000), num_workers=1,\n                total_time=20.0, parallel_efficiency=1.0, throughput=50.0\n            ),\n            ScalingResult(\n                \"strong_scaling\", 2, 1000, (2000, 2000), num_workers=2,\n                total_time=12.0, parallel_efficiency=0.8, throughput=83.0\n            )\n        ]\n        \n        report = benchmark.generate_scaling_report()\n        \n        assert \"SCALING ANALYSIS REPORT\" in report\n        assert \"WEAK SCALING RESULTS\" in report\n        assert \"STRONG SCALING RESULTS\" in report\n        assert \"RECOMMENDATIONS\" in report\n        assert \"Scale Factor\" in report\n        assert \"Workers\" in report\n    \n    @patch('builtins.open', create=True)\n    def test_export_results_json(self, mock_open):\n        \"\"\"Test JSON export of scaling results.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        result = ScalingResult(\n            \"weak_scaling\", 2, 200, (1400, 1400)\n        )\n        benchmark.results = [result]\n        \n        mock_file = Mock()\n        mock_open.return_value.__enter__.return_value = mock_file\n        \n        benchmark.export_results(\"scaling.json\", \"json\")\n        \n        mock_open.assert_called_once_with(\"scaling.json\", 'w')\n        mock_file.write.assert_called()  # JSON should be written\n    \n    def test_export_results_invalid_format(self):\n        \"\"\"Test export with invalid format.\"\"\"\n        benchmark = ScalingBenchmark()\n        \n        with pytest.raises(ValueError):\n            benchmark.export_results(\"test.txt\", \"invalid\")\n\n\nclass TestBenchmarkIntegration:\n    \"\"\"Integration tests for benchmark components.\"\"\"\n    \n    def test_standard_benchmark_with_real_agents(self):\n        \"\"\"Test standard benchmark with real agent classes.\"\"\"\n        config = SwarmConfig(num_agents=5, episode_length=20, seed=42)\n        benchmark = StandardBenchmark(\n            base_config=config,\n            num_episodes=1,\n            num_seeds=1,\n            parallel_execution=False\n        )\n        \n        # Run benchmark with real agent classes\n        results = benchmark.run_all(\n            agent_classes=[Agent, CooperativeAgent],\n            environments=[\"foraging_standard\"],\n            metrics=[\"reward\", \"efficiency\"]\n        )\n        \n        assert len(results) == 2  # 2 agents × 1 environment × 1 seed\n        \n        for result in results:\n            assert isinstance(result, BenchmarkResult)\n            assert result.agent_name in [\"Agent\", \"CooperativeAgent\"]\n            assert result.environment_name == \"ForagingEnvironment\"\n            assert result.execution_time > 0\n    \n    def test_scaling_benchmark_with_real_simulation(self):\n        \"\"\"Test scaling benchmark with real simulation.\"\"\"\n        config = SwarmConfig(episode_length=10, seed=42)\n        benchmark = ScalingBenchmark(\n            base_config=config,\n            episode_length=10,\n            warmup_steps=2\n        )\n        \n        # Test with small scale factors to keep test fast\n        results = benchmark.weak_scaling_test(\n            base_agents=5,\n            base_arena=100,\n            scale_factors=[1, 2],\n            metric=\"throughput\"\n        )\n        \n        assert len(results) == 2\n        \n        for result in results:\n            assert isinstance(result, ScalingResult)\n            assert result.test_type == \"weak_scaling\"\n            assert result.total_time > 0\n            assert result.throughput > 0\n    \n    def test_benchmark_error_handling(self):\n        \"\"\"Test benchmark error handling.\"\"\"\n        config = SwarmConfig(num_agents=1, episode_length=5)\n        benchmark = StandardBenchmark(\n            base_config=config,\n            num_episodes=1,\n            num_seeds=1\n        )\n        \n        # Create agent class that will fail\n        class FailingAgent(Agent):\n            def act(self, observation):\n                raise Exception(\"Intentional failure\")\n        \n        # Should handle failure gracefully\n        result = benchmark.run_single_benchmark(\n            FailingAgent,\n            Mock,\n            seed=42\n        )\n        \n        # Should return a failed result with zero values\n        assert result.agent_name == \"FailingAgent\"\n        assert result.mean_reward == 0.0\n        assert result.execution_time == 0.0