#!/usr/bin/env python3
"""
Comprehensive Quality Validation Suite

This validation suite tests all three generations of enhancements:
- Generation 1: Basic functionality and research capabilities  
- Generation 2: Robustness and security features
- Generation 3: Performance and scaling optimizations
"""

import sys
import time
import json
import traceback
import numpy as np
from typing import Dict, List, Any, Optional

# Import core components
try:
    from swarm_arena import Arena, SwarmConfig, set_global_seed
    from swarm_arena.core.agent import CooperativeAgent, CompetitiveAgent
    from swarm_arena.exceptions import ValidationError, ConfigurationError
    CORE_IMPORTS_OK = True
except Exception as e:
    print(f"âŒ Core imports failed: {e}")
    CORE_IMPORTS_OK = False

# Import Generation 1 features (Research)
try:
    from swarm_arena.research.breakthrough_algorithms import BreakthroughAlgorithms
    from swarm_arena.research.neural_swarm_intelligence import NeuralSwarmIntelligence, SwarmIntelligenceConfig
    GEN1_IMPORTS_OK = True
except Exception as e:
    print(f"âš ï¸  Generation 1 imports failed: {e}")
    GEN1_IMPORTS_OK = False

# Import Generation 2 features (Robustness)
try:
    from swarm_arena.resilience.quantum_error_correction import QuantumErrorCorrector
    from swarm_arena.security.advanced_authentication import AdvancedAuthenticator
    GEN2_IMPORTS_OK = True
except Exception as e:
    print(f"âš ï¸  Generation 2 imports failed: {e}")
    GEN2_IMPORTS_OK = False

# Import Generation 3 features (Performance)
try:
    from swarm_arena.optimization.quantum_computing_interface import QuantumOptimizer
    from swarm_arena.optimization.neuromorphic_processing import NeuromorphicSwarmProcessor
    GEN3_IMPORTS_OK = True
except Exception as e:
    print(f"âš ï¸  Generation 3 imports failed: {e}")
    GEN3_IMPORTS_OK = False

class QualityValidator:
    """Comprehensive quality validation for all system generations."""
    
    def __init__(self):
        self.test_results = {}
        self.overall_score = 0.0
        set_global_seed(42)  # Ensure reproducible tests
        
    def run_all_validations(self) -> Dict[str, Any]:
        """Run comprehensive validation suite."""
        
        print("ğŸ” COMPREHENSIVE QUALITY VALIDATION SUITE")
        print("=" * 50)
        
        start_time = time.time()
        
        # Core System Validation
        print("\nğŸ“‹ CORE SYSTEM VALIDATION")
        print("-" * 30)
        core_results = self.validate_core_functionality()
        self.test_results['core_system'] = core_results
        
        # Generation 1 Validation (Research)
        print("\nğŸ”¬ GENERATION 1 VALIDATION (Research Capabilities)")
        print("-" * 55)
        gen1_results = self.validate_generation1_research()
        self.test_results['generation_1_research'] = gen1_results
        
        # Generation 2 Validation (Robustness)
        print("\nğŸ›¡ï¸  GENERATION 2 VALIDATION (Robustness & Security)")
        print("-" * 56)
        gen2_results = self.validate_generation2_robustness()
        self.test_results['generation_2_robustness'] = gen2_results
        
        # Generation 3 Validation (Performance)
        print("\nğŸš€ GENERATION 3 VALIDATION (Performance & Scaling)")
        print("-" * 55)
        gen3_results = self.validate_generation3_performance()
        self.test_results['generation_3_performance'] = gen3_results
        
        # Integration Testing
        print("\nğŸ”— INTEGRATION TESTING")
        print("-" * 25)
        integration_results = self.validate_integration()
        self.test_results['integration'] = integration_results
        
        # Calculate overall score
        execution_time = time.time() - start_time
        self.overall_score = self.calculate_overall_score()
        
        # Generate final report
        final_report = self.generate_final_report(execution_time)
        
        return final_report
    
    def validate_core_functionality(self) -> Dict[str, Any]:
        """Validate core SwarmArena functionality."""
        
        if not CORE_IMPORTS_OK:
            return {
                'status': 'failed',
                'tests_passed': 0,
                'tests_total': 5,
                'error': 'Core imports failed'
            }
        
        tests = []
        
        # Test 1: Configuration validation
        try:
            # Valid config
            config = SwarmConfig(num_agents=10, arena_size=(100, 100))
            assert config.num_agents == 10
            tests.append(('config_validation', True))
            print("âœ… Configuration validation passed")
        except Exception as e:
            tests.append(('config_validation', False))
            print(f"âŒ Configuration validation failed: {e}")
        
        # Test 2: Arena creation
        try:
            config = SwarmConfig(num_agents=5, arena_size=(100, 100), episode_length=10)
            arena = Arena(config)
            assert len(arena.agents) == 0  # No agents added yet
            tests.append(('arena_creation', True))
            print("âœ… Arena creation passed")
        except Exception as e:
            tests.append(('arena_creation', False))
            print(f"âŒ Arena creation failed: {e}")
        
        # Test 3: Agent creation and addition
        try:
            arena.add_agents(CooperativeAgent, count=3)
            arena.add_agents(CompetitiveAgent, count=2)
            assert len(arena.agents) == 5
            tests.append(('agent_creation', True))
            print("âœ… Agent creation passed")
        except Exception as e:
            tests.append(('agent_creation', False))
            print(f"âŒ Agent creation failed: {e}")
        
        # Test 4: Basic simulation
        try:
            results = arena.evaluate(num_episodes=1, record_trajectories=False)
            assert results.total_steps > 0
            tests.append(('basic_simulation', True))
            print("âœ… Basic simulation passed")
        except Exception as e:
            tests.append(('basic_simulation', False))
            print(f"âŒ Basic simulation failed: {e}")
        
        # Test 5: Error handling
        try:
            try:
                SwarmConfig(num_agents=-1)  # Should raise ValidationError
                tests.append(('error_handling', False))
            except (ValidationError, ValueError):
                tests.append(('error_handling', True))
                print("âœ… Error handling passed")
        except Exception as e:
            tests.append(('error_handling', False))
            print(f"âŒ Error handling failed: {e}")
        
        passed_tests = sum(1 for _, passed in tests if passed)
        total_tests = len(tests)
        
        return {
            'status': 'passed' if passed_tests == total_tests else 'partial',
            'tests_passed': passed_tests,
            'tests_total': total_tests,
            'success_rate': passed_tests / total_tests,
            'individual_tests': dict(tests)
        }
    
    def validate_generation1_research(self) -> Dict[str, Any]:
        """Validate Generation 1 research capabilities."""
        
        if not GEN1_IMPORTS_OK:
            return {
                'status': 'failed',
                'tests_passed': 0,
                'tests_total': 4,
                'error': 'Generation 1 imports failed'
            }
        
        tests = []
        
        # Test 1: Breakthrough algorithms initialization
        try:
            breakthrough_algo = BreakthroughAlgorithms(significance_threshold=0.05)
            assert breakthrough_algo.significance_threshold == 0.05
            tests.append(('breakthrough_algorithms_init', True))
            print("âœ… Breakthrough algorithms initialization passed")
        except Exception as e:
            tests.append(('breakthrough_algorithms_init', False))
            print(f"âŒ Breakthrough algorithms initialization failed: {e}")
        
        # Test 2: Causal discovery
        try:
            # Create test trajectory data
            agent_trajectories = {
                1: np.random.rand(50, 2),
                2: np.random.rand(50, 2),
                3: np.random.rand(50, 2)
            }
            
            causal_graph = breakthrough_algo.discover_causal_structure(
                agent_trajectories, time_window=20
            )
            assert hasattr(causal_graph, 'nodes')
            assert hasattr(causal_graph, 'edges')
            tests.append(('causal_discovery', True))
            print("âœ… Causal discovery passed")
        except Exception as e:
            tests.append(('causal_discovery', False))
            print(f"âŒ Causal discovery failed: {e}")
        
        # Test 3: Neural swarm intelligence
        try:
            config = SwarmIntelligenceConfig(
                embedding_dim=32,  # Smaller for testing
                num_heads=2,
                num_layers=2,
                max_agents=10
            )
            neural_swarm = NeuralSwarmIntelligence(config)
            assert neural_swarm.config.embedding_dim == 32
            tests.append(('neural_swarm_init', True))
            print("âœ… Neural swarm intelligence initialization passed")
        except Exception as e:
            tests.append(('neural_swarm_init', False))
            print(f"âŒ Neural swarm intelligence initialization failed: {e}")
        
        # Test 4: Quantum fairness analysis
        try:
            agent_rewards = {
                1: [1.0, 2.0, 1.5],
                2: [1.2, 1.8, 1.6],
                3: [0.8, 2.2, 1.4]
            }
            agent_contributions = {
                1: [1.0, 1.5, 1.2],
                2: [1.1, 1.4, 1.3],
                3: [0.9, 1.6, 1.1]
            }
            
            fairness_results = breakthrough_algo.quantum_fairness_analysis(
                agent_rewards, agent_contributions
            )
            assert 'quantum_fairness_score' in fairness_results
            tests.append(('quantum_fairness', True))
            print("âœ… Quantum fairness analysis passed")
        except Exception as e:
            tests.append(('quantum_fairness', False))
            print(f"âŒ Quantum fairness analysis failed: {e}")
        
        passed_tests = sum(1 for _, passed in tests if passed)
        total_tests = len(tests)
        
        return {
            'status': 'passed' if passed_tests == total_tests else 'partial',
            'tests_passed': passed_tests,
            'tests_total': total_tests,
            'success_rate': passed_tests / total_tests,
            'individual_tests': dict(tests)
        }
    
    def validate_generation2_robustness(self) -> Dict[str, Any]:
        """Validate Generation 2 robustness and security features."""
        
        if not GEN2_IMPORTS_OK:
            return {
                'status': 'failed',
                'tests_passed': 0,
                'tests_total': 4,
                'error': 'Generation 2 imports failed'
            }
        
        tests = []
        
        # Test 1: Quantum error correction
        try:
            corrector = QuantumErrorCorrector(redundancy_level=2)
            
            # Test encoding
            test_data = np.array([[1.0, 2.0], [3.0, 4.0]])
            quantum_state = corrector.encode_quantum_state(test_data)
            
            # Test error detection and correction
            corrupted_data = test_data + np.random.normal(0, 0.1, test_data.shape)
            correction = corrector.detect_and_correct_errors(quantum_state, corrupted_data)
            
            assert correction.errors_detected >= 0
            assert correction.confidence >= 0
            tests.append(('quantum_error_correction', True))
            print("âœ… Quantum error correction passed")
        except Exception as e:
            tests.append(('quantum_error_correction', False))
            print(f"âŒ Quantum error correction failed: {e}")
        
        # Test 2: Advanced authentication
        try:
            authenticator = AdvancedAuthenticator()
            
            # Generate credentials
            agent_creds = authenticator.generate_agent_credentials(1)
            assert 'agent_id' in agent_creds
            assert 'secret' in agent_creds
            
            # Test authentication
            auth_token = authenticator.authenticate_agent(
                agent_id=1,
                credentials={'secret': agent_creds['secret']},
                requested_permissions=['read_basic']
            )
            assert auth_token is not None
            tests.append(('advanced_authentication', True))
            print("âœ… Advanced authentication passed")
        except Exception as e:
            tests.append(('advanced_authentication', False))
            print(f"âŒ Advanced authentication failed: {e}")
        
        # Test 3: Trust level management
        try:
            # Update trust levels
            authenticator.update_agent_trust(1, +10, "test_increase")
            authenticator.update_agent_trust(1, -5, "test_decrease")
            
            # Get security report
            security_report = authenticator.get_security_report()
            assert 'trust_distribution' in security_report
            tests.append(('trust_management', True))
            print("âœ… Trust level management passed")
        except Exception as e:
            tests.append(('trust_management', False))
            print(f"âŒ Trust level management failed: {e}")
        
        # Test 4: Token validation
        try:
            # Validate token
            if auth_token:
                validated_token = authenticator.validate_token(auth_token.token_id)
                assert validated_token is not None
                
                # Test token revocation
                authenticator.revoke_agent_tokens(1)
                revoked_token = authenticator.validate_token(auth_token.token_id)
                assert revoked_token is None
                
            tests.append(('token_validation', True))
            print("âœ… Token validation passed")
        except Exception as e:
            tests.append(('token_validation', False))
            print(f"âŒ Token validation failed: {e}")
        
        passed_tests = sum(1 for _, passed in tests if passed)
        total_tests = len(tests)
        
        return {
            'status': 'passed' if passed_tests == total_tests else 'partial',
            'tests_passed': passed_tests,
            'tests_total': total_tests,
            'success_rate': passed_tests / total_tests,
            'individual_tests': dict(tests)
        }
    
    def validate_generation3_performance(self) -> Dict[str, Any]:
        """Validate Generation 3 performance and scaling features."""
        
        if not GEN3_IMPORTS_OK:
            return {
                'status': 'failed',
                'tests_passed': 0,
                'tests_total': 4,
                'error': 'Generation 3 imports failed'
            }
        
        tests = []
        
        # Test 1: Quantum optimization
        try:
            quantum_optimizer = QuantumOptimizer(backend="simulator")
            
            # Test problem
            agent_positions = {
                1: np.array([0.0, 0.0]),
                2: np.array([10.0, 0.0]),
                3: np.array([5.0, 10.0])
            }
            target_formation = np.array([
                [0.0, 0.0],
                [20.0, 0.0],
                [10.0, 20.0]
            ])
            
            result = quantum_optimizer.solve_agent_coordination(
                agent_positions, target_formation
            )
            
            assert result.quantum_advantage >= 1.0
            tests.append(('quantum_optimization', True))
            print("âœ… Quantum optimization passed")
        except Exception as e:
            tests.append(('quantum_optimization', False))
            print(f"âŒ Quantum optimization failed: {e}")
        
        # Test 2: Neuromorphic processing
        try:
            processor = NeuromorphicSwarmProcessor(max_agents=20, processing_cores=2)
            
            # Test data
            agent_positions = {
                1: np.array([1.0, 1.0]),
                2: np.array([2.0, 2.0])
            }
            agent_velocities = {
                1: np.array([0.1, 0.2]),
                2: np.array([0.2, 0.1])
            }
            
            result = processor.process_swarm_behavior(
                agent_positions, agent_velocities
            )
            
            assert 'behavioral_analysis' in result
            assert 'energy_consumption' in result
            tests.append(('neuromorphic_processing', True))
            print("âœ… Neuromorphic processing passed")
        except Exception as e:
            tests.append(('neuromorphic_processing', False))
            print(f"âŒ Neuromorphic processing failed: {e}")
        
        # Test 3: Performance metrics
        try:
            # Test performance report
            performance_report = processor.get_performance_report()
            assert 'total_operations' in performance_report
            
            # Test efficiency metrics
            efficiency = result['efficiency_metrics']
            assert 'energy_per_agent' in efficiency
            tests.append(('performance_metrics', True))
            print("âœ… Performance metrics passed")
        except Exception as e:
            tests.append(('performance_metrics', False))
            print(f"âŒ Performance metrics failed: {e}")
        
        # Test 4: Scaling validation
        try:
            # Test with larger agent count
            large_positions = {i: np.random.rand(2) for i in range(10)}
            large_velocities = {i: np.random.rand(2) for i in range(10)}
            
            large_result = processor.process_swarm_behavior(
                large_positions, large_velocities
            )
            
            # Should handle larger problems
            assert large_result['agents_processed'] == 10
            tests.append(('scaling_validation', True))
            print("âœ… Scaling validation passed")
        except Exception as e:
            tests.append(('scaling_validation', False))
            print(f"âŒ Scaling validation failed: {e}")
        
        passed_tests = sum(1 for _, passed in tests if passed)
        total_tests = len(tests)
        
        return {
            'status': 'passed' if passed_tests == total_tests else 'partial',
            'tests_passed': passed_tests,
            'tests_total': total_tests,
            'success_rate': passed_tests / total_tests,
            'individual_tests': dict(tests)
        }
    
    def validate_integration(self) -> Dict[str, Any]:
        """Validate integration between all generations."""
        
        tests = []
        
        # Test 1: Core + Generation 1 integration
        try:
            if CORE_IMPORTS_OK and GEN1_IMPORTS_OK:
                # Create arena with research capabilities
                config = SwarmConfig(num_agents=5, arena_size=(100, 100), episode_length=20)
                arena = Arena(config)
                arena.add_agents(CooperativeAgent, count=5)
                
                # Run simulation and collect data
                results = arena.evaluate(num_episodes=1, record_trajectories=True)
                
                # Use research tools on results
                breakthrough_algo = BreakthroughAlgorithms()
                
                # Test data extraction from arena
                assert results.total_steps > 0
                tests.append(('core_research_integration', True))
                print("âœ… Core + Research integration passed")
            else:
                tests.append(('core_research_integration', False))
                print("âŒ Core + Research integration skipped (missing imports)")
        except Exception as e:
            tests.append(('core_research_integration', False))
            print(f"âŒ Core + Research integration failed: {e}")
        
        # Test 2: Security + Performance integration
        try:
            if GEN2_IMPORTS_OK and GEN3_IMPORTS_OK:
                # Test secure quantum optimization
                authenticator = AdvancedAuthenticator()
                quantum_optimizer = QuantumOptimizer()
                
                # Generate agent credentials
                creds = authenticator.generate_agent_credentials(1)
                
                # Authenticate for optimization
                token = authenticator.authenticate_agent(
                    1, {'secret': creds['secret']}, ['read_basic']
                )
                
                if token:
                    # Perform optimization
                    test_positions = {1: np.array([0.0, 0.0])}
                    test_target = np.array([[1.0, 1.0]])
                    
                    result = quantum_optimizer.solve_agent_coordination(
                        test_positions, test_target
                    )
                    assert result.quantum_advantage >= 1.0
                
                tests.append(('security_performance_integration', True))
                print("âœ… Security + Performance integration passed")
            else:
                tests.append(('security_performance_integration', False))
                print("âŒ Security + Performance integration skipped (missing imports)")
        except Exception as e:
            tests.append(('security_performance_integration', False))
            print(f"âŒ Security + Performance integration failed: {e}")
        
        # Test 3: Full system integration
        try:
            if CORE_IMPORTS_OK:
                # Test system resilience
                config = SwarmConfig(num_agents=3, arena_size=(50, 50), episode_length=10)
                arena = Arena(config)
                arena.add_agents(CooperativeAgent, count=3)
                
                # Multiple evaluation runs
                for i in range(3):
                    results = arena.evaluate(num_episodes=1)
                    assert results.total_steps > 0
                
                tests.append(('full_system_integration', True))
                print("âœ… Full system integration passed")
            else:
                tests.append(('full_system_integration', False))
                print("âŒ Full system integration skipped (core imports failed)")
        except Exception as e:
            tests.append(('full_system_integration', False))
            print(f"âŒ Full system integration failed: {e}")
        
        passed_tests = sum(1 for _, passed in tests if passed)
        total_tests = len(tests)
        
        return {
            'status': 'passed' if passed_tests == total_tests else 'partial',
            'tests_passed': passed_tests,
            'tests_total': total_tests,
            'success_rate': passed_tests / total_tests,
            'individual_tests': dict(tests)
        }
    
    def calculate_overall_score(self) -> float:
        """Calculate overall quality score."""
        
        # Weight different test categories
        weights = {
            'core_system': 0.4,          # Core functionality is critical
            'generation_1_research': 0.2,   # Research features
            'generation_2_robustness': 0.2, # Security and robustness
            'generation_3_performance': 0.15, # Performance features
            'integration': 0.05           # Integration testing
        }
        
        total_score = 0.0
        total_weight = 0.0
        
        for category, weight in weights.items():
            if category in self.test_results:
                category_score = self.test_results[category].get('success_rate', 0.0)
                total_score += category_score * weight
                total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def generate_final_report(self, execution_time: float) -> Dict[str, Any]:
        """Generate comprehensive final report."""
        
        # Count totals
        total_tests_passed = sum(r.get('tests_passed', 0) for r in self.test_results.values())
        total_tests_total = sum(r.get('tests_total', 0) for r in self.test_results.values())
        
        # Determine overall status
        if self.overall_score >= 0.9:
            status = "EXCELLENT"
            emoji = "ğŸ†"
        elif self.overall_score >= 0.8:
            status = "GOOD"
            emoji = "âœ…"
        elif self.overall_score >= 0.6:
            status = "ACCEPTABLE"
            emoji = "âš ï¸"
        else:
            status = "NEEDS_IMPROVEMENT"
            emoji = "âŒ"
        
        # Print summary
        print("\n" + "=" * 60)
        print("ğŸ“Š COMPREHENSIVE QUALITY VALIDATION REPORT")
        print("=" * 60)
        
        print(f"\nğŸ¯ OVERALL SCORE: {self.overall_score:.1%}")
        print(f"ğŸ“ˆ STATUS: {emoji} {status}")
        print(f"â±ï¸  EXECUTION TIME: {execution_time:.2f}s")
        print(f"ğŸ“‹ TESTS: {total_tests_passed}/{total_tests_total} passed")
        
        print("\nğŸ“Š DETAILED RESULTS:")
        for category, results in self.test_results.items():
            success_rate = results.get('success_rate', 0.0)
            tests_passed = results.get('tests_passed', 0)
            tests_total = results.get('tests_total', 0)
            
            status_emoji = "âœ…" if success_rate >= 0.8 else "âš ï¸" if success_rate >= 0.6 else "âŒ"
            
            print(f"  {status_emoji} {category.replace('_', ' ').title()}: "
                  f"{success_rate:.1%} ({tests_passed}/{tests_total})")
        
        # Feature availability
        print("\nğŸ”§ FEATURE AVAILABILITY:")
        print(f"  Core System: {'âœ…' if CORE_IMPORTS_OK else 'âŒ'}")
        print(f"  Research Features (Gen 1): {'âœ…' if GEN1_IMPORTS_OK else 'âŒ'}")
        print(f"  Security Features (Gen 2): {'âœ…' if GEN2_IMPORTS_OK else 'âŒ'}")
        print(f"  Performance Features (Gen 3): {'âœ…' if GEN3_IMPORTS_OK else 'âŒ'}")
        
        print("\nğŸ¯ RECOMMENDATIONS:")
        if self.overall_score >= 0.9:
            print("  â€¢ System is production-ready")
            print("  â€¢ All major features operational")
            print("  â€¢ Continue monitoring and optimization")
        elif self.overall_score >= 0.8:
            print("  â€¢ System is near production-ready")
            print("  â€¢ Minor improvements recommended")
            print("  â€¢ Focus on failed test categories")
        else:
            print("  â€¢ System needs additional development")
            print("  â€¢ Address failed tests before production")
            print("  â€¢ Consider incremental deployment")
        
        # Compile final report
        final_report = {
            "validation_timestamp": time.time(),
            "execution_time": execution_time,
            "overall_score": self.overall_score,
            "overall_status": status,
            "tests_summary": {
                "total_passed": total_tests_passed,
                "total_tests": total_tests_total,
                "success_rate": total_tests_passed / max(total_tests_total, 1)
            },
            "feature_availability": {
                "core_system": CORE_IMPORTS_OK,
                "generation_1_research": GEN1_IMPORTS_OK,
                "generation_2_robustness": GEN2_IMPORTS_OK,
                "generation_3_performance": GEN3_IMPORTS_OK
            },
            "detailed_results": self.test_results,
            "recommendations": self._generate_recommendations()
        }
        
        return final_report
    
    def _generate_recommendations(self) -> List[str]:
        """Generate specific recommendations based on test results."""
        recommendations = []
        
        # Check each category
        for category, results in self.test_results.items():
            success_rate = results.get('success_rate', 0.0)
            
            if success_rate < 0.8:
                if category == 'core_system':
                    recommendations.append("Critical: Address core system failures immediately")
                elif category == 'generation_1_research':
                    recommendations.append("Research features need attention - check imports and dependencies")
                elif category == 'generation_2_robustness':
                    recommendations.append("Security and robustness features require fixes")
                elif category == 'generation_3_performance':
                    recommendations.append("Performance optimization features need debugging")
                elif category == 'integration':
                    recommendations.append("Integration issues detected - review component compatibility")
        
        # General recommendations
        if self.overall_score < 0.6:
            recommendations.append("Overall system quality below acceptable threshold")
        
        if not recommendations:
            recommendations.append("System performing well - continue current development practices")
        
        return recommendations

def main():
    """Main execution function."""
    
    try:
        # Run comprehensive validation
        validator = QualityValidator()
        final_report = validator.run_all_validations()
        
        # Save detailed report
        report_file = f"quality_validation_report_{int(time.time())}.json"
        with open(report_file, 'w') as f:
            json.dump(final_report, f, indent=2, default=str)
        
        print(f"\nğŸ“ Detailed report saved to: {report_file}")
        
        # Exit with appropriate code
        exit_code = 0 if final_report['overall_score'] >= 0.8 else 1
        print(f"\nğŸ Validation completed with exit code: {exit_code}")
        
        return exit_code
        
    except Exception as e:
        print(f"\nğŸ’¥ Validation suite crashed: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        return 2

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)