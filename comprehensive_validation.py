#!/usr/bin/env python3
"""
Comprehensive Quality Gates and Testing
Final validation before production deployment.
"""

import sys
sys.path.insert(0, '.')

import time
import traceback
import psutil
import os
from pathlib import Path

def run_quality_gates():
    """Run comprehensive quality gates."""
    print("üîí SWARM ARENA - COMPREHENSIVE QUALITY GATES")
    print("=" * 60)
    
    results = {}
    total_tests = 0
    passed_tests = 0
    
    # Gate 1: Code Import and Basic Functionality
    print("\nüõ†Ô∏è GATE 1: Core Functionality")
    print("-" * 40)
    try:
        from swarm_arena import Arena, SwarmConfig, Agent
        from swarm_arena.core.agent import CooperativeAgent, CompetitiveAgent
        from swarm_arena.monitoring.telemetry import TelemetryCollector
        from swarm_arena.benchmarks.standard import StandardBenchmark
        print("‚úÖ All core imports successful")
        
        # Basic configuration test
        config = SwarmConfig(num_agents=10, episode_length=50, seed=42)
        arena = Arena(config)
        arena.add_agents(Agent, 5)
        arena.add_agents(CooperativeAgent, 5)
        print("‚úÖ Basic arena setup successful")
        
        results["core_functionality"] = True
        passed_tests += 1
    except Exception as e:
        print(f"‚ùå Core functionality failed: {e}")
        results["core_functionality"] = False
    total_tests += 1
    
    # Gate 2: Performance Under Load
    print("\n‚ö° GATE 2: Performance Under Load")
    print("-" * 40)
    try:
        # Test with moderate load
        config = SwarmConfig(
            num_agents=100,
            episode_length=100,
            arena_size=(800, 600),
            seed=42
        )
        arena = Arena(config)
        arena.add_agents(CooperativeAgent, 50)
        arena.add_agents(CompetitiveAgent, 50)
        
        # Performance measurement
        start_time = time.time()
        results_obj = arena.run(episodes=1, verbose=False)
        execution_time = time.time() - start_time
        
        steps_per_second = config.episode_length / execution_time
        agent_steps_per_second = steps_per_second * config.num_agents
        
        print(f"‚úÖ Performance test completed")
        print(f"   ‚Ä¢ Execution time: {execution_time:.2f}s")
        print(f"   ‚Ä¢ Steps/second: {steps_per_second:.0f}")
        print(f"   ‚Ä¢ Agent-steps/second: {agent_steps_per_second:.0f}")
        print(f"   ‚Ä¢ Mean reward: {results_obj.mean_reward:.3f}")
        
        # Performance thresholds
        if steps_per_second >= 10 and agent_steps_per_second >= 1000:
            print("‚úÖ Performance meets requirements")
            results["performance"] = True
            passed_tests += 1
        else:
            print("‚ö†Ô∏è Performance below thresholds")
            results["performance"] = False
            
    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")
        results["performance"] = False
    total_tests += 1
    
    # Gate 3: Memory Management
    print("\nüíæ GATE 3: Memory Management")
    print("-" * 40)
    try:
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Run memory-intensive test
        config = SwarmConfig(num_agents=200, episode_length=50, seed=42)
        arena = Arena(config)
        arena.add_agents(Agent, 200)
        
        before_memory = process.memory_info().rss / 1024 / 1024
        results_obj = arena.run(episodes=1, verbose=False)
        after_memory = process.memory_info().rss / 1024 / 1024
        
        memory_growth = after_memory - initial_memory
        memory_per_agent = memory_growth / config.num_agents
        
        print(f"‚úÖ Memory test completed")
        print(f"   ‚Ä¢ Initial memory: {initial_memory:.1f} MB")
        print(f"   ‚Ä¢ Final memory: {after_memory:.1f} MB")
        print(f"   ‚Ä¢ Memory growth: {memory_growth:.1f} MB")
        print(f"   ‚Ä¢ Memory per agent: {memory_per_agent:.3f} MB")
        
        # Memory thresholds
        if memory_per_agent < 1.0 and memory_growth < 500:
            print("‚úÖ Memory usage within limits")
            results["memory"] = True
            passed_tests += 1
        else:
            print("‚ö†Ô∏è Memory usage exceeds limits")
            results["memory"] = False
            
    except Exception as e:
        print(f"‚ùå Memory test failed: {e}")
        results["memory"] = False
    total_tests += 1
    
    # Gate 4: Error Resilience
    print("\nüõ°Ô∏è GATE 4: Error Resilience")
    print("-" * 40)
    error_tests = 0
    error_passed = 0
    
    # Test invalid configurations
    try:
        SwarmConfig(num_agents=0)
        print("‚ùå Should reject num_agents=0")
    except:
        print("‚úÖ Configuration validation working")
        error_passed += 1
    error_tests += 1
    
    try:
        SwarmConfig(num_agents=10, episode_length=-5)
        print("‚ùå Should reject negative episode_length")
    except:
        print("‚úÖ Episode length validation working")
        error_passed += 1
    error_tests += 1
    
    try:
        SwarmConfig(resource_spawn_rate=1.5)
        print("‚ùå Should reject spawn_rate > 1.0")
    except:
        print("‚úÖ Spawn rate validation working")
        error_passed += 1
    error_tests += 1
    
    # Test agent error handling
    try:
        agent = Agent(0, [100, 200])
        invalid_obs = {"invalid": "observation"}
        action = agent.act(invalid_obs)  # Should handle gracefully
        print("‚úÖ Agent error handling working")
        error_passed += 1
    except Exception as e:
        print(f"‚ö†Ô∏è Agent error handling needs improvement: {e}")
    error_tests += 1
    
    if error_passed >= error_tests * 0.8:  # 80% threshold
        print("‚úÖ Error resilience meets requirements")
        results["error_resilience"] = True
        passed_tests += 1
    else:
        print("‚ö†Ô∏è Error resilience needs improvement")
        results["error_resilience"] = False
    total_tests += 1
    
    # Gate 5: Multi-Agent Behavior
    print("\nü§ñ GATE 5: Multi-Agent Behavior")
    print("-" * 40)
    try:
        config = SwarmConfig(num_agents=50, episode_length=100, seed=42)
        arena = Arena(config)
        
        # Add diverse agent types
        arena.add_agents(CooperativeAgent, 15)
        arena.add_agents(CompetitiveAgent, 15)
        
        # Import advanced agents if available
        try:
            from swarm_arena.core.agent import SwarmAgent, AdaptiveAgent, HierarchicalAgent
            arena.add_agents(SwarmAgent, 10)
            arena.add_agents(AdaptiveAgent, 5)
            arena.add_agents(HierarchicalAgent, 5)
            advanced_agents = True
        except ImportError:
            arena.add_agents(Agent, 20)
            advanced_agents = False
        
        results_obj = arena.run(episodes=1, verbose=False)
        
        # Analyze behavior diversity
        agent_rewards = []
        for agent_rewards_list in results_obj.episode_rewards.values():
            agent_rewards.extend(agent_rewards_list)
        
        reward_std = float('inf')
        if agent_rewards:
            import statistics
            reward_std = statistics.stdev(agent_rewards) if len(agent_rewards) > 1 else 0
        
        print(f"‚úÖ Multi-agent simulation completed")
        print(f"   ‚Ä¢ Agent types: {'Advanced' if advanced_agents else 'Basic'}")
        print(f"   ‚Ä¢ Mean reward: {results_obj.mean_reward:.3f}")
        print(f"   ‚Ä¢ Reward diversity: {reward_std:.3f}")
        print(f"   ‚Ä¢ Fairness index: {results_obj.fairness_index:.3f}")
        
        # Behavior validation
        if results_obj.mean_reward > 0 and results_obj.fairness_index > 0.5:
            print("‚úÖ Multi-agent behavior meets requirements")
            results["multi_agent"] = True
            passed_tests += 1
        else:
            print("‚ö†Ô∏è Multi-agent behavior needs improvement")
            results["multi_agent"] = False
            
    except Exception as e:
        print(f"‚ùå Multi-agent test failed: {e}")
        traceback.print_exc()
        results["multi_agent"] = False
    total_tests += 1
    
    # Gate 6: Integration and Monitoring
    print("\nüìä GATE 6: Monitoring and Integration")
    print("-" * 40)
    try:
        # Test telemetry
        telemetry = TelemetryCollector(auto_start=False)
        telemetry.update_telemetry(step=10, fps=30.0, active_agents=50)
        data = telemetry.get_latest_data()
        
        if data and data.step == 10 and data.fps == 30.0:
            print("‚úÖ Telemetry system working")
        
        # Test health monitoring
        try:
            from swarm_arena.monitoring.health import HealthMonitor
            health = HealthMonitor(auto_start=False)
            health_status = health.check_system_health()
            if health_status and 'status' in health_status:
                print("‚úÖ Health monitoring working")
            health.cleanup()
        except Exception as e:
            print(f"‚ö†Ô∏è Health monitoring issue: {e}")
        
        # Test benchmarking
        try:
            benchmark = StandardBenchmark(num_episodes=1, num_seeds=1, parallel_execution=False)
            print("‚úÖ Benchmarking system available")
        except Exception as e:
            print(f"‚ö†Ô∏è Benchmarking issue: {e}")
        
        results["monitoring"] = True
        passed_tests += 1
        
    except Exception as e:
        print(f"‚ùå Monitoring test failed: {e}")
        results["monitoring"] = False
    total_tests += 1
    
    # Final Assessment
    print(f"\n" + "=" * 60)
    print(f"üéØ QUALITY GATES ASSESSMENT")
    print("=" * 60)
    print(f"Total gates: {total_tests}")
    print(f"Passed gates: {passed_tests}")
    print(f"Success rate: {(passed_tests/total_tests)*100:.1f}%")
    
    print(f"\nüìã Gate Results:")
    for gate, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"   {gate:<20} {status}")
    
    # Production readiness assessment
    print(f"\nüöÄ PRODUCTION READINESS:")
    if passed_tests == total_tests:
        print("‚úÖ FULLY PRODUCTION READY")
        print("   ‚Ä¢ All quality gates passed")
        print("   ‚Ä¢ System meets all requirements") 
        print("   ‚Ä¢ Ready for deployment")
        readiness = "READY"
    elif passed_tests >= total_tests * 0.85:  # 85% threshold
        print("‚úÖ PRODUCTION READY WITH MINOR ISSUES")
        print("   ‚Ä¢ Most quality gates passed")
        print("   ‚Ä¢ Minor issues can be addressed post-deployment")
        print("   ‚Ä¢ Proceed with caution")
        readiness = "READY_WITH_ISSUES"
    else:
        print("‚ö†Ô∏è NOT PRODUCTION READY")
        print("   ‚Ä¢ Critical quality gates failed")
        print("   ‚Ä¢ Issues must be resolved before deployment")
        print("   ‚Ä¢ Additional development required")
        readiness = "NOT_READY"
    
    print(f"\nüìä QUALITY METRICS SUMMARY:")
    print(f"   ‚Ä¢ Core functionality: {'‚úì' if results.get('core_functionality') else '‚úó'}")
    print(f"   ‚Ä¢ Performance: {'‚úì' if results.get('performance') else '‚úó'}")
    print(f"   ‚Ä¢ Memory management: {'‚úì' if results.get('memory') else '‚úó'}")
    print(f"   ‚Ä¢ Error resilience: {'‚úì' if results.get('error_resilience') else '‚úó'}")
    print(f"   ‚Ä¢ Multi-agent behavior: {'‚úì' if results.get('multi_agent') else '‚úó'}")
    print(f"   ‚Ä¢ Monitoring integration: {'‚úì' if results.get('monitoring') else '‚úó'}")
    
    return readiness, passed_tests, total_tests

if __name__ == "__main__":
    readiness, passed, total = run_quality_gates()
    
    if readiness == "READY":
        sys.exit(0)
    elif readiness == "READY_WITH_ISSUES":
        sys.exit(1)
    else:
        sys.exit(2)