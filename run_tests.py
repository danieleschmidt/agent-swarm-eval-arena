#!/usr/bin/env python3\n\"\"\"Simple test runner for Swarm Arena.\"\"\"\n\nimport sys\nimport traceback\nimport time\nfrom pathlib import Path\n\n# Add repo to path\nsys.path.insert(0, str(Path(__file__).parent))\n\n\ndef run_basic_tests():\n    \"\"\"Run basic functionality tests.\"\"\"\n    print(\"ğŸ§ª Running Swarm Arena Tests\")\n    print(\"=\" * 50)\n    \n    test_results = {}\n    total_tests = 0\n    passed_tests = 0\n    \n    # Test 1: Basic imports\n    try:\n        print(\"\\nğŸ“¦ Testing imports...\")\n        from swarm_arena import Arena, SwarmConfig, Agent\n        from swarm_arena.core.agent import CooperativeAgent, CompetitiveAgent\n        from swarm_arena.core.environment import ForagingEnvironment\n        from swarm_arena.utils.seeding import set_global_seed\n        print(\"  âœ… Core imports successful\")\n        test_results[\"imports\"] = True\n        passed_tests += 1\n    except Exception as e:\n        print(f\"  âŒ Import failed: {str(e)}\")\n        test_results[\"imports\"] = False\n    total_tests += 1\n    \n    # Test 2: Configuration validation\n    try:\n        print(\"\\nâš™ï¸  Testing configuration...\")\n        config = SwarmConfig(num_agents=10, episode_length=50, seed=42)\n        assert config.num_agents == 10\n        assert config.episode_length == 50\n        assert config.seed == 42\n        \n        # Test validation\n        try:\n            SwarmConfig(num_agents=0)  # Should fail\n            print(\"  âŒ Configuration validation failed (should reject num_agents=0)\")\n            test_results[\"config\"] = False\n        except:\n            print(\"  âœ… Configuration validation working\")\n            test_results[\"config\"] = True\n            passed_tests += 1\n    except Exception as e:\n        print(f\"  âŒ Configuration test failed: {str(e)}\")\n        test_results[\"config\"] = False\n    total_tests += 1\n    \n    # Test 3: Agent creation and basic functionality\n    try:\n        print(\"\\nğŸ¤– Testing agents...\")\n        agent = Agent(agent_id=0, initial_position=[100, 200])\n        assert agent.agent_id == 0\n        assert len(agent.state.position) == 2\n        \n        # Test action\n        observation = {\n            \"position\": [100, 200],\n            \"resources\": [[150, 250]],\n            \"nearby_agents\": [],\n            \"arena_bounds\": {\"width\": 1000, \"height\": 1000}\n        }\n        action = agent.act(observation)\n        assert 0 <= action <= 5\n        \n        # Test different agent types\n        coop_agent = CooperativeAgent(1, [200, 300])\n        comp_agent = CompetitiveAgent(2, [300, 400])\n        \n        print(\"  âœ… Agent creation and basic functionality working\")\n        test_results[\"agents\"] = True\n        passed_tests += 1\n    except Exception as e:\n        print(f\"  âŒ Agent test failed: {str(e)}\")\n        traceback.print_exc()\n        test_results[\"agents\"] = False\n    total_tests += 1\n    \n    # Test 4: Environment functionality\n    try:\n        print(\"\\nğŸŒ Testing environment...\")\n        config = SwarmConfig(num_agents=5, episode_length=20, seed=42)\n        env = ForagingEnvironment(config)\n        \n        # Test reset\n        env_state = env.reset()\n        assert \"environment\" in env_state\n        assert len(env.state.resources) >= 1\n        \n        # Test step\n        actions = {0: 1, 1: 2, 2: 0}\n        obs, rewards, done, info = env.step(actions)\n        assert isinstance(rewards, dict)\n        assert isinstance(done, bool)\n        \n        print(\"  âœ… Environment functionality working\")\n        test_results[\"environment\"] = True\n        passed_tests += 1\n    except Exception as e:\n        print(f\"  âŒ Environment test failed: {str(e)}\")\n        traceback.print_exc()\n        test_results[\"environment\"] = False\n    total_tests += 1\n    \n    # Test 5: Arena simulation\n    try:\n        print(\"\\nğŸŸï¸  Testing arena simulation...\")\n        config = SwarmConfig(num_agents=5, episode_length=10, seed=42)\n        arena = Arena(config)\n        \n        # Add agents\n        arena.add_agents(Agent, count=3)\n        arena.add_agents(CooperativeAgent, count=2)\n        \n        assert len(arena.agents) == 5\n        \n        # Run simulation\n        results = arena.run(episodes=1, verbose=False)\n        \n        assert results.mean_reward is not None\n        assert len(results.agent_stats) == 5\n        assert results.total_steps > 0\n        \n        print(\"  âœ… Arena simulation working\")\n        test_results[\"arena\"] = True\n        passed_tests += 1\n    except Exception as e:\n        print(f\"  âŒ Arena test failed: {str(e)}\")\n        traceback.print_exc()\n        test_results[\"arena\"] = False\n    total_tests += 1\n    \n    # Test 6: Monitoring components\n    try:\n        print(\"\\nğŸ“Š Testing monitoring...\")\n        from swarm_arena.monitoring.telemetry import TelemetryCollector, TelemetryData\n        from swarm_arena.utils.monitoring import PerformanceMonitor\n        \n        # Test telemetry data\n        data = TelemetryData(step=10, active_agents=5)\n        assert data.step == 10\n        assert data.active_agents == 5\n        \n        # Test telemetry collector\n        collector = TelemetryCollector(auto_start=False)\n        collector.update_telemetry(step=5, fps=30.0)\n        latest = collector.get_latest_data()\n        assert latest.step == 5\n        assert latest.fps == 30.0\n        \n        # Test performance monitor\n        monitor = PerformanceMonitor()\n        metrics = monitor.record_metrics(fps=60.0, step_time=0.01)\n        assert metrics.fps == 60.0\n        \n        print(\"  âœ… Monitoring components working\")\n        test_results[\"monitoring\"] = True\n        passed_tests += 1\n    except Exception as e:\n        print(f\"  âŒ Monitoring test failed: {str(e)}\")\n        traceback.print_exc()\n        test_results[\"monitoring\"] = False\n    total_tests += 1\n    \n    # Test 7: Benchmarking components\n    try:\n        print(\"\\nğŸ“ˆ Testing benchmarking...\")\n        from swarm_arena.benchmarks.standard import StandardBenchmark, BenchmarkResult\n        from swarm_arena.benchmarks.scaling import ScalingBenchmark, ScalingResult\n        \n        # Test benchmark result\n        result = BenchmarkResult(\n            \"TestAgent\", \"TestEnv\", 42, 1.5, 0.3, 0.8, 0.7, 0.9, 0.6, 1000, 5.2\n        )\n        assert result.agent_name == \"TestAgent\"\n        assert result.mean_reward == 1.5\n        \n        # Test scaling result\n        scaling_result = ScalingResult(\n            \"weak_scaling\", 2, 200, (1000, 1000)\n        )\n        assert scaling_result.test_type == \"weak_scaling\"\n        \n        # Test benchmark initialization\n        benchmark = StandardBenchmark(num_episodes=1, num_seeds=1, parallel_execution=False)\n        assert benchmark.num_episodes == 1\n        \n        print(\"  âœ… Benchmarking components working\")\n        test_results[\"benchmarking\"] = True\n        passed_tests += 1\n    except Exception as e:\n        print(f\"  âŒ Benchmarking test failed: {str(e)}\")\n        traceback.print_exc()\n        test_results[\"benchmarking\"] = False\n    total_tests += 1\n    \n    # Test 8: Validation utilities\n    try:\n        print(\"\\nâœ… Testing validation...\")\n        from swarm_arena.utils.validation import (\n            validate_positive, validate_probability, validate_position, StateValidator\n        )\n        from swarm_arena.exceptions import ValidationError\n        \n        # Test basic validators\n        validate_positive(1.0, \"test\")\n        validate_probability(0.5, \"test\")\n        validate_position([100, 200], (500, 400), \"test\")\n        \n        # Test validation error\n        try:\n            validate_positive(-1, \"test\")\n            print(\"  âŒ Validation should have failed for negative value\")\n            test_results[\"validation\"] = False\n        except ValidationError:\n            print(\"  âœ… Validation utilities working\")\n            test_results[\"validation\"] = True\n            passed_tests += 1\n    except Exception as e:\n        print(f\"  âŒ Validation test failed: {str(e)}\")\n        traceback.print_exc()\n        test_results[\"validation\"] = False\n    total_tests += 1\n    \n    # Test 9: Example scripts\n    try:\n        print(\"\\nğŸ“ Testing example scripts...\")\n        \n        # Test basic usage example (import only)\n        import importlib.util\n        spec = importlib.util.spec_from_file_location(\n            \"basic_usage\", \n            Path(__file__).parent / \"examples\" / \"basic_usage.py\"\n        )\n        if spec and spec.loader:\n            basic_usage = importlib.util.module_from_spec(spec)\n            # Just test that it can be imported without syntax errors\n            print(\"  âœ… Example scripts syntax valid\")\n            test_results[\"examples\"] = True\n            passed_tests += 1\n        else:\n            print(\"  âŒ Could not load example scripts\")\n            test_results[\"examples\"] = False\n    except Exception as e:\n        print(f\"  âŒ Example scripts test failed: {str(e)}\")\n        test_results[\"examples\"] = False\n    total_tests += 1\n    \n    # Print summary\n    print(\"\\n\" + \"=\" * 50)\n    print(f\"ğŸ“Š TEST SUMMARY\")\n    print(f\"Total tests: {total_tests}\")\n    print(f\"Passed: {passed_tests}\")\n    print(f\"Failed: {total_tests - passed_tests}\")\n    print(f\"Success rate: {passed_tests/total_tests*100:.1f}%\")\n    \n    print(\"\\nğŸ“‹ Detailed Results:\")\n    for test_name, result in test_results.items():\n        status = \"âœ… PASS\" if result else \"âŒ FAIL\"\n        print(f\"  {test_name:<15} {status}\")\n    \n    if passed_tests == total_tests:\n        print(\"\\nğŸ‰ All tests passed!\")\n        return True\n    else:\n        print(f\"\\nâš ï¸  {total_tests - passed_tests} tests failed\")\n        return False\n\n\nif __name__ == \"__main__\":\n    success = run_basic_tests()\n    sys.exit(0 if success else 1)